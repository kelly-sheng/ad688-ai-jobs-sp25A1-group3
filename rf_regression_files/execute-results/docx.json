{
  "hash": "ddcc5e70ff77847f47a65a129bf1a0ad",
  "result": {
    "engine": "jupyter",
    "markdown": "::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lit, concat_ws, lower, regexp_replace, when, mean as _mean\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Start Spark ---\nspark = SparkSession.builder.appName(\"AI_Job_Comparison\").getOrCreate()\n\n# --- Load data ---\ndf = (spark.read.option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(\"data/lightcast_job_postings.csv\"))\n\n# --- Create text features for AI labeling ---\ntext_cols = [\"TITLE_RAW\", \"TITLE_CLEAN\", \"BODY\", \"SKILLS_NAME\",\n             \"COMMON_SKILLS_NAME\", \"SPECIALIZED_SKILLS_NAME\", \n             \"SOFTWARE_SKILLS_NAME\"]\n\ndf = df.fillna(\"\")\ndf = df.withColumn(\"text_features\",\n                   concat_ws(\" \", *[lower(regexp_replace(col(c), r\"[^a-zA-Z0-9 ]\", \"\")) for c in text_cols]))\n\n# --- AI job label ---\nai_keywords = [\n    r\"\\bAI\\b\", r\"\\bML\\b\", r\"\\bLLM\\b\", r\"\\bNLP\\b\",\n    \"artificial intelligence\", \"machine learning\", \"deep learning\",\n    \"computer vision\", \"generative\", \"gen ai\", \"chatgpt\", r\"gpt-\\d+\",\n    \"transformer\", \"bert\", \"prompt engineer\", \"reinforcement learning\"\n]\nai_pattern = \"|\".join([f\"(?i){k}\" for k in ai_keywords])\ndf = df.withColumn(\"requires_ai\", when(col(\"text_features\").rlike(ai_pattern), lit(1)).otherwise(lit(0)))\n\n# --- Create average salary ---\ndf = df.withColumn(\n    \"SALARY_AVG\",\n    when(col(\"SALARY\") > 0, col(\"SALARY\"))\n    .when(col(\"SALARY_FROM\").isNotNull() & col(\"SALARY_TO\").isNotNull(),\n          (col(\"SALARY_FROM\") + col(\"SALARY_TO\")) / 2)\n    .when(col(\"SALARY_FROM\").isNotNull(), col(\"SALARY_FROM\"))\n    .when(col(\"SALARY_TO\").isNotNull(), col(\"SALARY_TO\"))\n    .otherwise(None)\n)\n\n# --- Handle education level ---\ndf = df.withColumn(\"MIN_EDULEVELS\", when(col(\"MIN_EDULEVELS\") == 99, 0).otherwise(col(\"MIN_EDULEVELS\")))\n\n# --- Select relevant features ---\nmodel_df = df.select(\n    \"requires_ai\",\n    \"SALARY_AVG\",\n    \"MIN_YEARS_EXPERIENCE\",\n    \"MIN_EDULEVELS\",\n    \"STATE\",\n    \"REMOTE_TYPE\"\n)\n\n# --- Fill numeric nulls ---\nnumeric_cols = [\"SALARY_AVG\", \"MIN_YEARS_EXPERIENCE\", \"MIN_EDULEVELS\"]\nfor c in numeric_cols:\n    model_df = model_df.withColumn(c, col(c).cast(DoubleType()))\n    mean_val = model_df.select(_mean(col(c))).first()[0]\n    model_df = model_df.na.fill({c: mean_val})\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 54:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 55:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 58:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 61:>                                                         (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# --- Encode categorical variables ---\ncategorical_cols = [\"STATE\", \"REMOTE_TYPE\"]\nindexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_IDX\", handleInvalid=\"keep\") for c in categorical_cols]\nencoders = [OneHotEncoder(inputCol=f\"{c}_IDX\", outputCol=f\"{c}_VEC\") for c in categorical_cols]\n\n# --- Assemble features ---\nassembler = VectorAssembler(\n    inputCols=numeric_cols + [f\"{c}_VEC\" for c in categorical_cols],\n    outputCol=\"features\"\n)\n\n# --- Random Forest classifier ---\nrf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"requires_ai\", numTrees=50, maxDepth=5, seed=42)\n\n# --- Build pipeline ---\npipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n\n# --- Split train/test ---\ntrain_df, test_df = model_df.randomSplit([0.8, 0.2], seed=42)\n\n# --- Train model ---\nrf_model = pipeline.fit(train_df)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 64:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 70:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 76:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 79:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 80:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 81:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 83:>                                                         (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# --- Predict & evaluate ---\npredictions = rf_model.transform(test_df)\nevaluator = BinaryClassificationEvaluator(labelCol=\"requires_ai\", metricName=\"areaUnderROC\")\nroc_auc = evaluator.evaluate(predictions)\nprint(f\"ROC-AUC: {roc_auc:.3f}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 93:>                                                         (0 + 1) / 1]\r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nROC-AUC: 0.646\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r                                                                                \r\n```\n:::\n:::\n\n\nROC-AUC was used instead of accuracy because it is more suited for binary classifications such as AI vs non-AI.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# --- Feature importances (aggregate categorical vectors) ---\nrf_stage = rf_model.stages[-1]\nimportances = rf_stage.featureImportances.toArray()\n\n# Get the sizes of one-hot encoded vectors\nohe_stages = [s for s in rf_model.stages if isinstance(s, OneHotEncoder)]\ncategorical_sizes = []\nfor ohe in ohe_stages:\n    # Number of categories = size of metadata after encoding\n    categorical_sizes.append(len(ohe.getOutputCol() + \"_metadata\") if False else ohe.getOutputCols() if hasattr(ohe,'getOutputCols') else ohe.categorySizes[0] if hasattr(ohe,'categorySizes') else None)\n\n# Alternative simpler approach: we can get vector size from the first row of transformed vector\nfeatures_vec = rf_model.transform(test_df).select(\"features\").first()[0]\nvector_size = len(features_vec)\n\n# Approximate: numeric features = 1, categorical features = remaining size divided equally\nnum_numeric = len(numeric_cols)\nnum_categorical = len(categorical_cols)\ncategorical_vector_sizes = [(vector_size - num_numeric)//num_categorical]*num_categorical\n\nfeature_sizes = [1]*num_numeric + categorical_vector_sizes\nfeature_names = numeric_cols + categorical_cols\n\n# Aggregate importances\nagg_importances = []\nstart = 0\nfor size in feature_sizes:\n    agg_importances.append(np.sum(importances[start:start+size]))\n    start += size\n\n# --- Create DataFrame & plot ---\nimportance_df = pd.DataFrame({\n    \"Feature\": feature_names,\n    \"Importance\": agg_importances\n}).sort_values(\"Importance\", ascending=False)\n\nplt.figure(figsize=(8,5))\nplt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\nplt.xlabel(\"Importance\")\nplt.title(\"Random Forest Feature Importances\")\nplt.gca().invert_yaxis()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 102:>                                                        (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](rf_regression_files/figure-docx/cell-5-output-2.png){}\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# --- Predict sample job listings using the trained pipeline ---\nfrom pyspark.ml.functions import vector_to_array\n\nsample_jobs = [\n    Row(MIN_YEARS_EXPERIENCE=15.0, SALARY_AVG=300000.0, STATE=\"NY\", MIN_EDULEVELS=4.0, REMOTE_TYPE=\"Remote\"),\n    Row(MIN_YEARS_EXPERIENCE=2.0, SALARY_AVG=75000.0, STATE=\"DE\", MIN_EDULEVELS=0.0, REMOTE_TYPE=\"Onsite\"),\n    Row(MIN_YEARS_EXPERIENCE=3.0, SALARY_AVG=95000.0, STATE=\"ND\", MIN_EDULEVELS=1.0, REMOTE_TYPE=\"Hybrid\")\n]\n\nsample_df = spark.createDataFrame(sample_jobs)\n\n# Ensure same schema types\n\nfor c in [\"MIN_YEARS_EXPERIENCE\", \"SALARY_AVG\", \"MIN_EDULEVELS\"]:\n    sample_df = sample_df.withColumn(c, col(c).cast(DoubleType()))\n\n# --- Use the *same pipeline* to transform data ---\n\npredictions = rf_model.transform(sample_df)\npredictions_array = predictions.withColumn(\"prob_array\", vector_to_array(col(\"probability\")))\n\n# --- Extract probability of AI class and display results ---\n\nresults = predictions_array.select(\n    \"MIN_YEARS_EXPERIENCE\",\n    \"SALARY_AVG\",\n    \"STATE\",\n    \"MIN_EDULEVELS\",\n    \"REMOTE_TYPE\",\n    col(\"prob_array\")[1].alias(\"AI_prob\"),\n    \"prediction\"\n)\n\nresults.show(truncate=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------+----------+-----+-------------+-----------+-------------------+----------+\n|MIN_YEARS_EXPERIENCE|SALARY_AVG|STATE|MIN_EDULEVELS|REMOTE_TYPE|AI_prob            |prediction|\n+--------------------+----------+-----+-------------+-----------+-------------------+----------+\n|15.0                |300000.0  |NY   |4.0          |Remote     |0.3676113258332611 |0.0       |\n|2.0                 |75000.0   |DE   |0.0          |Onsite     |0.19717699753401854|0.0       |\n|3.0                 |95000.0   |ND   |1.0          |Hybrid     |0.19611136141019866|0.0       |\n+--------------------+----------+-----+-------------+-----------+-------------------+----------+\n\n```\n:::\n:::\n\n\n# Random Forest Regression Summary\n\nThis analysis used a Random Forest model to distinguish AI from non-AI job postings based on five features: minimum years of experience, average salary, state, minimum education level, and remote work type. These features were chosen because AI roles typically require more experience, offer higher compensation, demand advanced degrees, and are concentrated in specific regions, while remote flexibility can influence applicant reach. The model revealed that years of experience and salary were the most influential factors, followed by state, education, and remote type, reflecting the expected patterns of AI job characteristics.\n\nHowever, when using this model to predict whether or not a posting would be AI or not, we encountered several limitations that may explain its underperformance. Treating state as a numeric variable may not fully capture the categorical differences between regions, and the modelâ€™s majority-vote mechanism can classify borderline AI roles as non-AI, especially when AI postings are underrepresented in the training data. Additional factors such as inconsistent labeling of AI roles, limited sample size, and high variance in salary or education requirements could further reduce predictive accuracy. Despite these challenges, the analysis highlights which features are most informative for distinguishing AI roles and provides a foundation for future models.\n\n",
    "supporting": [
      "rf_regression_files/figure-docx"
    ],
    "filters": []
  }
}