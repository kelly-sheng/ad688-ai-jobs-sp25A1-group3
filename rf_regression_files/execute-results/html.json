{
  "hash": "1bfee808fde05c4925a0b6942a61454e",
  "result": {
    "engine": "jupyter",
    "markdown": "::: {#8139a737 .cell execution_count=1}\n``` {.python .cell-code}\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.sql.functions import col, lit, concat_ws, lower, regexp_replace, when, trim, mean as _mean\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.functions import vector_to_array\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Start Spark ---\nspark = SparkSession.builder.appName(\"AI_Job_Comparison\").getOrCreate()\n\n# --- Load data ---\ndf = (spark.read.option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(\"data/lightcast_job_postings.csv\"))\n\n# --- Create text features for AI labeling ---\ntext_cols = [\"TITLE_RAW\", \"TITLE_CLEAN\", \"BODY\", \"SKILLS_NAME\",\n             \"COMMON_SKILLS_NAME\", \"SPECIALIZED_SKILLS_NAME\", \n             \"SOFTWARE_SKILLS_NAME\"]\n\ndf = df.fillna(\"\")\ndf = df.withColumn(\"text_features\",\n                   concat_ws(\" \", *[lower(regexp_replace(col(c), r\"[^a-zA-Z0-9 ]\", \"\")) for c in text_cols]))\n\n# --- AI job label ---\nai_keywords = [\n    r\"\\bAI\\b\", r\"\\bML\\b\", r\"\\bLLM\\b\", r\"\\bNLP\\b\",\n    \"artificial intelligence\", \"machine learning\", \"deep learning\",\n    \"computer vision\", \"generative\", \"gen ai\", \"chatgpt\", r\"gpt-\\d+\",\n    \"transformer\", \"bert\", \"prompt engineer\", \"reinforcement learning\"\n]\nai_pattern = \"|\".join([f\"(?i){k}\" for k in ai_keywords])\ndf = df.withColumn(\"requires_ai\", when(col(\"text_features\").rlike(ai_pattern), lit(1)).otherwise(lit(0)))\n\n# --- Create average salary ---\ndf = df.withColumn(\n    \"SALARY_AVG\",\n    when(col(\"SALARY\") > 0, col(\"SALARY\"))\n    .when(col(\"SALARY_FROM\").isNotNull() & col(\"SALARY_TO\").isNotNull(),\n          (col(\"SALARY_FROM\") + col(\"SALARY_TO\")) / 2)\n    .when(col(\"SALARY_FROM\").isNotNull(), col(\"SALARY_FROM\"))\n    .when(col(\"SALARY_TO\").isNotNull(), col(\"SALARY_TO\"))\n    .otherwise(None)\n)\n\n# --- Handle education level ---\ndf = df.withColumn(\"MIN_EDULEVELS\", when(col(\"MIN_EDULEVELS\") == 99, 0).otherwise(col(\"MIN_EDULEVELS\")))\n\n# --- Select relevant features ---\nmodel_df = df.select(\n    \"requires_ai\",\n    \"SALARY_AVG\",\n    \"MIN_YEARS_EXPERIENCE\",\n    \"MIN_EDULEVELS\",\n    \"STATE_NAME\",\n    \"REMOTE_TYPE\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n25/10/14 23:12:36 WARN Utils: Your hostname, Kellys-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.6 instead (on interface en0)\n25/10/14 23:12:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/10/14 23:12:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\r[Stage 1:>                                                          (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {#f4d44779 .cell execution_count=2}\n``` {.python .cell-code}\n# --- Handle numeric nulls ---\nnumeric_cols = [\"SALARY_AVG\", \"MIN_YEARS_EXPERIENCE\", \"MIN_EDULEVELS\"]\nfor c in numeric_cols:\n    model_df = model_df.withColumn(c, col(c).cast(DoubleType()))\n    mean_val = model_df.select(_mean(col(c))).first()[0]\n    model_df = model_df.na.fill({c: mean_val})\n\n# --- Handle categorical nulls ---\ncategorical_cols = [\"STATE_NAME\", \"REMOTE_TYPE\"]\nmodel_df = model_df.filter(\n    (trim(col(\"STATE_NAME\")) != \"\") &\n    (trim(col(\"REMOTE_TYPE\")) != \"\") &\n    col(\"STATE_NAME\").isNotNull() &\n    col(\"REMOTE_TYPE\").isNotNull()\n)\n\n# --- Encode categorical variables ---\nindexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_IDX\", handleInvalid=\"keep\") for c in categorical_cols]\nencoders = [OneHotEncoder(inputCol=f\"{c}_IDX\", outputCol=f\"{c}_VEC\") for c in categorical_cols]\n\n# --- Assemble numeric features ---\nnumeric_assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"numeric_features\")\n\n# --- Scale numeric features to 0-1 ---\nscaler = MinMaxScaler(inputCol=\"numeric_features\", outputCol=\"numeric_scaled\")\n\n# --- Assemble final feature vector (scaled numeric + categorical vectors) ---\nfinal_assembler = VectorAssembler(\n    inputCols=[\"numeric_scaled\"] + [f\"{c}_VEC\" for c in categorical_cols],\n    outputCol=\"features\"\n)\n\n# --- Logistic Regression classifier ---\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"requires_ai\", maxIter=50, regParam=0.0, elasticNetParam=0.0)\n\n# --- Build pipeline ---\npipeline = Pipeline(stages=indexers + encoders + [numeric_assembler, scaler, final_assembler, lr])\n\n# --- Split train/test and fit ---\ntrain_df, test_df = model_df.randomSplit([0.8, 0.2], seed=42)\nlr_model = pipeline.fit(train_df)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 2:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 5:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 8:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 11:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 17:>                                                         (0 + 1) / 1]\r\r                                                                                \r\r[Stage 23:>                                                         (0 + 1) / 1]\r\r                                                                                \r25/10/14 23:14:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n\r[Stage 26:>                                                         (0 + 1) / 1]\r\r                                                                                \r25/10/14 23:14:48 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n\r[Stage 27:>                                                         (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {#2872b48d .cell execution_count=3}\n``` {.python .cell-code}\n# --- Predict & evaluate ---\npredictions = lr_model.transform(test_df)\nevaluator = BinaryClassificationEvaluator(labelCol=\"requires_ai\", metricName=\"areaUnderROC\")\nroc_auc = evaluator.evaluate(predictions)\nprint(f\"ROC-AUC: {roc_auc:.3f}\")\n\n# --- Extract logistic regression stage ---\nlr_stage = lr_model.stages[-1]\n\n# LogisticRegressionModel in Spark stores coefficients in a dense vector\ncoefficients = lr_stage.coefficients.toArray()\n\n# --- Compute feature sizes ---\n# Number of numeric features\nnum_numeric = len(numeric_cols)\n# One-hot vector sizes for categorical features\nfeatures_vec = lr_model.transform(test_df).select(\"features\").first()[0]\nvector_size = len(features_vec)\nnum_categorical = len(categorical_cols)\ncategorical_vector_sizes = [(vector_size - num_numeric)//num_categorical]*num_categorical\n\n# --- Aggregate coefficients by feature ---\nfeature_sizes = [1]*num_numeric + categorical_vector_sizes\nfeature_names = numeric_cols + categorical_cols\n\nagg_importances = []\nstart = 0\nfor size in feature_sizes:\n    # Take mean of absolute values of coefficients for categorical features\n    agg_importances.append(np.mean(np.abs(coefficients[start:start+size])))\n    start += size\n\n# --- Create DataFrame & plot ---\nimportance_df = pd.DataFrame({\n    \"Feature\": feature_names,\n    \"Importance\": agg_importances\n}).sort_values(\"Importance\", ascending=False)\n\nplt.figure(figsize=(8,5))\nplt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\nplt.xlabel(\"Mean Absolute Coefficient\")\nplt.title(\"Logistic Regression Feature Importances (mean per categorical vector)\")\nplt.gca().invert_yaxis()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 38:>                                                         (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nROC-AUC: 0.623\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 47:>                                                         (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](rf_regression_files/figure-html/cell-4-output-4.png){width=789 height=449}\n:::\n:::\n\n\nROC-AUC was used instead of accuracy because it is more suited for binary classifications such as AI vs non-AI.\n\n::: {#7894b1b0 .cell execution_count=4}\n``` {.python .cell-code}\n# --- Create sample job listings ---\nsample_jobs = [\n    Row(MIN_YEARS_EXPERIENCE=15.0, SALARY_AVG=300000.0, STATE_NAME=\"California\", MIN_EDULEVELS=3.0, REMOTE_TYPE=\"Remote\"),\n    Row(MIN_YEARS_EXPERIENCE=5.0, SALARY_AVG=75000.0, STATE_NAME=\"Texas\", MIN_EDULEVELS=0.0, REMOTE_TYPE=\"Onsite\"),\n    Row(MIN_YEARS_EXPERIENCE=10.0, SALARY_AVG=300000.0, STATE_NAME=\"New York\", MIN_EDULEVELS=2.0, REMOTE_TYPE=\"Hybrid\")\n]\n\nsample_df = spark.createDataFrame(sample_jobs)\n\n# Ensure numeric columns are DoubleType\nfor c in [\"MIN_YEARS_EXPERIENCE\", \"SALARY_AVG\", \"MIN_EDULEVELS\"]:\n    sample_df = sample_df.withColumn(c, col(c).cast(DoubleType()))\n\n# --- Transform samples through the trained pipeline ---\npredictions = lr_model.transform(sample_df)\n\n# --- Extract probability of AI class ---\npredictions_array = predictions.withColumn(\"prob_array\", vector_to_array(col(\"probability\")))\n\n# --- Show results ---\nresults = predictions_array.select(\n    \"MIN_YEARS_EXPERIENCE\",\n    \"SALARY_AVG\",\n    \"STATE_NAME\",\n    \"MIN_EDULEVELS\",\n    \"REMOTE_TYPE\",\n    col(\"prob_array\")[1].alias(\"AI_prob\"),\n    \"prediction\"\n)\n\nresults.show(truncate=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------+----------+----------+-------------+-----------+-------------------+----------+\n|MIN_YEARS_EXPERIENCE|SALARY_AVG|STATE_NAME|MIN_EDULEVELS|REMOTE_TYPE|AI_prob            |prediction|\n+--------------------+----------+----------+-------------+-----------+-------------------+----------+\n|15.0                |300000.0  |California|3.0          |Remote     |0.7072587987227981 |1.0       |\n|5.0                 |75000.0   |Texas     |0.0          |Onsite     |0.23680753160930024|0.0       |\n|10.0                |300000.0  |New York  |2.0          |Hybrid     |0.5660738033071526 |1.0       |\n+--------------------+----------+----------+-------------+-----------+-------------------+----------+\n\n```\n:::\n:::\n\n\n# Random Forest Regression Summary\n\nThis analysis skimmed the job listings for keywords to distinguish AI from non-AI job postings and used a logistic regression model trained on five features: minimum years of experience, average salary, state, minimum education level, and remote work type. These features were chosen because AI roles typically require more experience, offer higher compensation, demand advanced degrees, and are concentrated in specific regions, while remote flexibility can influence applicant reach. The model revealed that years of experience and salary were the most influential factors, followed by state, education, and remote type, reflecting the expected patterns of AI job characteristics.\n\nIn order to create the model, we encoded the categorical variables and used MinMaxScaler to size the continuous variables to a 0-1 scale, which allowed the training model to weigh each variable equally. After obtaining the model data, we then charted the feature importance, making sure to take the aggregate coefficients to avoid over-reporting the importance of the encoded variables. The model reveals that salary is the most important feature when determining whether or not a role involves AI. When using the model against a sample data set, it predicted that the $300,000 salary job in Massachusetts with 10 years of minimum experience would involve AI, but the $150,000 salary job in New York with 15 minimum years of experience would not.\n\nIn order to refine its predictive capabilities, this model could benefit from some fine-tuning; right now it is evaluating probability and deeming any data with over 50% chance as involving AI, but if we were to get more granular with the data, the ideal threshold to use in this process may be closer to 35% or 40%. Additionally, given the ever-changing landscape of AI and the continued trend of AI becoming commonplace in many jobs, that threshold may continue to lower with time.\n\n",
    "supporting": [
      "rf_regression_files"
    ],
    "filters": [],
    "includes": {}
  }
}