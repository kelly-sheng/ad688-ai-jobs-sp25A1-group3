
```{python}
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import col, lit, concat_ws, lower, regexp_replace, when, trim, mean as _mean
from pyspark.sql.types import DoubleType
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml import Pipeline
from pyspark.ml.functions import vector_to_array
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# --- Start Spark ---
spark = SparkSession.builder.appName("AI_Job_Comparison").getOrCreate()
spark.sparkContext.setLogLevel("FATAL")

# --- Load data ---
df = (spark.read.option("header", "true")
        .option("inferSchema", "true")
        .option("multiLine", "true")
        .option("escape", "\"")
        .csv("data/lightcast_job_postings.csv"))

# --- Create text features for AI labeling ---
text_cols = ["TITLE_RAW", "TITLE_CLEAN", "BODY", "SKILLS_NAME",
             "COMMON_SKILLS_NAME", "SPECIALIZED_SKILLS_NAME", 
             "SOFTWARE_SKILLS_NAME"]

df = df.fillna("")
df = df.withColumn("text_features",
                   concat_ws(" ", *[lower(regexp_replace(col(c), r"[^a-zA-Z0-9 ]", "")) for c in text_cols]))

# --- AI job label ---
ai_keywords = [
    r"\bAI\b", r"\bML\b", r"\bLLM\b", r"\bNLP\b",
    "artificial intelligence", "machine learning", "deep learning",
    "computer vision", "generative", "gen ai", "chatgpt", r"gpt-\d+",
    "transformer", "bert", "prompt engineer", "reinforcement learning"
]
ai_pattern = "|".join([f"(?i){k}" for k in ai_keywords])
df = df.withColumn("requires_ai", when(col("text_features").rlike(ai_pattern), lit(1)).otherwise(lit(0)))

# --- Handle education level ---
df = df.withColumn("MIN_EDULEVELS", when(col("MIN_EDULEVELS") == 99, 0).otherwise(col("MIN_EDULEVELS")))

# --- Select relevant features ---
model_df = df.select(
    "requires_ai",
    "NAICS2_NAME",
    "MIN_YEARS_EXPERIENCE",
    "MIN_EDULEVELS",
    "STATE_NAME",
    "REMOTE_TYPE_NAME"
)

# --- Handle numeric nulls ---
numeric_cols = ["MIN_YEARS_EXPERIENCE", "MIN_EDULEVELS"]
for c in numeric_cols:
    model_df = model_df.withColumn(c, col(c).cast(DoubleType()))
    mean_val = model_df.select(_mean(col(c))).first()[0]
    model_df = model_df.na.fill({c: mean_val})

# --- Handle categorical nulls ---
categorical_cols = ["NAICS2_NAME", "STATE_NAME", "REMOTE_TYPE_NAME"]
model_df = model_df.filter(
    (trim(col("STATE_NAME")) != "") &
    (trim(col("REMOTE_TYPE_NAME")) != "") &
    (trim(col("NAICS2_NAME")) != "") &
    col("STATE_NAME").isNotNull() &
    col("REMOTE_TYPE_NAME").isNotNull() &
    col("NAICS2_NAME").isNotNull()
)

# --- Encode categorical variables ---
indexers = [StringIndexer(inputCol=c, outputCol=f"{c}_IDX", handleInvalid="keep") for c in categorical_cols]
encoders = [OneHotEncoder(inputCol=f"{c}_IDX", outputCol=f"{c}_VEC") for c in categorical_cols]

# --- Assemble numeric features ---
numeric_assembler = VectorAssembler(inputCols=numeric_cols, outputCol="numeric_features")

# --- Scale numeric features to 0-1 ---
scaler = MinMaxScaler(inputCol="numeric_features", outputCol="numeric_scaled")

# --- Assemble final feature vector (scaled numeric + categorical vectors) ---
final_assembler = VectorAssembler(
    inputCols=["numeric_scaled"] + [f"{c}_VEC" for c in categorical_cols],
    outputCol="features"
)

# --- Logistic Regression classifier ---
from pyspark.ml.classification import LogisticRegression
lr = LogisticRegression(featuresCol="features", labelCol="requires_ai", maxIter=50, regParam=0.0, elasticNetParam=0.0)

# --- Build pipeline ---
pipeline = Pipeline(stages=indexers + encoders + [numeric_assembler, scaler, final_assembler, lr])

# --- Split train/test and fit ---
train_df, test_df = model_df.randomSplit([0.8, 0.2], seed=42)
lr_model = pipeline.fit(train_df)
```

```{python}
# --- Predict & evaluate ---
predictions = lr_model.transform(test_df)
evaluator = BinaryClassificationEvaluator(labelCol="requires_ai", metricName="areaUnderROC")
roc_auc = evaluator.evaluate(predictions)
print(f"ROC-AUC: {roc_auc:.3f}")

# --- Extract logistic regression stage ---
lr_stage = lr_model.stages[-1]

# LogisticRegressionModel in Spark stores coefficients in a dense vector
coefficients = lr_stage.coefficients.toArray()

# --- Compute feature sizes ---
# Number of numeric features
num_numeric = len(numeric_cols)
# One-hot vector sizes for categorical features
features_vec = lr_model.transform(test_df).select("features").first()[0]
vector_size = len(features_vec)
num_categorical = len(categorical_cols)
categorical_vector_sizes = [(vector_size - num_numeric)//num_categorical]*num_categorical

# --- Aggregate coefficients by feature ---
feature_sizes = [1]*num_numeric + categorical_vector_sizes
feature_names = numeric_cols + categorical_cols

agg_importances = []
start = 0
for size in feature_sizes:
    # Take mean of absolute values of coefficients for categorical features
    agg_importances.append(np.mean(np.abs(coefficients[start:start+size])))
    start += size

# --- Create DataFrame & plot ---
importance_df = pd.DataFrame({
    "Feature": feature_names,
    "Importance": agg_importances
}).sort_values("Importance", ascending=False)

plt.figure(figsize=(8,5))
plt.barh(importance_df["Feature"], importance_df["Importance"])
plt.xlabel("Mean Absolute Coefficient")
plt.title("Logistic Regression Feature Importances (mean per categorical vector)")
plt.gca().invert_yaxis()
plt.savefig("output/logistic_regression.png", dpi=200, bbox_inches="tight")
plt.show()
```
ROC-AUC was used instead of accuracy because it is more suited for binary classifications such as AI vs non-AI.

```{python}
# --- Create sample job listings ---
sample_jobs = [
    Row(MIN_YEARS_EXPERIENCE=15.0, NAICS2_NAME="Professional, Scientific, and Technical Services", STATE_NAME="New York", MIN_EDULEVELS=3.0, REMOTE_TYPE_NAME="Remote"),
    Row(MIN_YEARS_EXPERIENCE=5.0, NAICS2_NAME="Finance and Insurance", STATE_NAME="Texas", MIN_EDULEVELS=2.0, REMOTE_TYPE_NAME="Remote"),
    Row(MIN_YEARS_EXPERIENCE=10.0, NAICS2_NAME="Real Estate and Rental and Leasing", STATE_NAME="Minnesota", MIN_EDULEVELS=0.0, REMOTE_TYPE_NAME="Hybrid Remote"),
    Row(MIN_YEARS_EXPERIENCE=15.0, NAICS2_NAME="Health Care and Social Assistance", STATE_NAME="New York", MIN_EDULEVELS=4.0, REMOTE_TYPE_NAME="Not Remote"),
]

sample_df = spark.createDataFrame(sample_jobs)

# Ensure numeric columns are DoubleType
for c in ["MIN_YEARS_EXPERIENCE", "MIN_EDULEVELS"]:
    sample_df = sample_df.withColumn(c, col(c).cast(DoubleType()))

# --- Transform samples through the trained pipeline ---
predictions = lr_model.transform(sample_df)

# --- Extract probability of AI class ---
predictions_array = predictions.withColumn("prob_array", vector_to_array(col("probability")))

# --- Show results ---
results = predictions_array.select(
    "MIN_YEARS_EXPERIENCE",
    "NAICS2_NAME",
    "STATE_NAME",
    "MIN_EDULEVELS",
    "REMOTE_TYPE_NAME",
    col("prob_array")[1].alias("AI_prob"),
    "prediction"
)

results.show(truncate=False)
```

# Logistic Regression Summary

This analysis skimmed the job listings for keywords to distinguish AI from non-AI job postings and used a logistic regression model trained on five features: minimum years of experience, NAICS2 classification, state, minimum education level, and remote work type. These features were chosen because AI roles typically require more experience, are concentrated in certain industries over others, demand advanced degrees, and could be more prevalent in specific regions, while remote flexibility can influence applicant reach. The model revealed that years of experience was by far the most influential factor, followed by NAICS2, reflecting the expected patterns of AI job characteristics.

In order to create the model, we encoded the categorical variables and used MinMaxScaler to size the continuous variables to a 0-1 scale, which allowed the training model to weigh each variable equally. After obtaining the model data, we then charted the feature importance, making sure to take the aggregate coefficients to avoid over-reporting the importance of the encoded variables. The model reveals that years of experience is the most important feature when determining whether or not a role involves AI. However, the combination of all factors can lead to dramatically different results. For example, when using the model against a sample data set, it predicted that the 15-year remote listing in New York in "Professional, Scientific, and Technical Services" would be an AI job, but the 15-year on-site listing in New York in "Health Care and Social Assistance" would not.

In order to refine its predictive capabilities, this model could benefit from some fine-tuning; right now it is evaluating probability and deeming any data with over 50% chance as involving AI, but if we were to get more granular with the data, the ideal threshold to use in this process may be closer to 35% or 40%. Additionally, given the ever-changing landscape of AI and the continued trend of AI becoming commonplace in many jobs, that threshold may continue to lower with time. This tool should be consistently re-evaluated to maintain relevancy so that users can derive the most value from its results.