[
  {
    "objectID": "lr_regression.html",
    "href": "lr_regression.html",
    "title": "AI vs.Â Non-AI Careers",
    "section": "",
    "text": "from pyspark.sql import SparkSession, Row\nfrom pyspark.sql.functions import col, lit, concat_ws, lower, regexp_replace, when, trim, mean as _mean\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.functions import vector_to_array\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Start Spark ---\nspark = SparkSession.builder.appName(\"AI_Job_Comparison\").getOrCreate()\nspark.sparkContext.setLogLevel(\"FATAL\")\n\n# --- Load data ---\ndf = (spark.read.option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"multiLine\", \"true\")\n        .option(\"escape\", \"\\\"\")\n        .csv(\"data/lightcast_job_postings.csv\"))\n\n# --- Create text features for AI labeling ---\ntext_cols = [\"TITLE_RAW\", \"TITLE_CLEAN\", \"BODY\", \"SKILLS_NAME\",\n             \"COMMON_SKILLS_NAME\", \"SPECIALIZED_SKILLS_NAME\", \n             \"SOFTWARE_SKILLS_NAME\"]\n\ndf = df.fillna(\"\")\ndf = df.withColumn(\"text_features\",\n                   concat_ws(\" \", *[lower(regexp_replace(col(c), r\"[^a-zA-Z0-9 ]\", \"\")) for c in text_cols]))\n\n# --- AI job label ---\nai_keywords = [\n    r\"\\bAI\\b\", r\"\\bML\\b\", r\"\\bLLM\\b\", r\"\\bNLP\\b\",\n    \"artificial intelligence\", \"machine learning\", \"deep learning\",\n    \"computer vision\", \"generative\", \"gen ai\", \"chatgpt\", r\"gpt-\\d+\",\n    \"transformer\", \"bert\", \"prompt engineer\", \"reinforcement learning\"\n]\nai_pattern = \"|\".join([f\"(?i){k}\" for k in ai_keywords])\ndf = df.withColumn(\"requires_ai\", when(col(\"text_features\").rlike(ai_pattern), lit(1)).otherwise(lit(0)))\n\n# --- Handle education level ---\ndf = df.withColumn(\"MIN_EDULEVELS\", when(col(\"MIN_EDULEVELS\") == 99, 0).otherwise(col(\"MIN_EDULEVELS\")))\n\n# --- Select relevant features ---\nmodel_df = df.select(\n    \"requires_ai\",\n    \"NAICS2_NAME\",\n    \"MIN_YEARS_EXPERIENCE\",\n    \"MIN_EDULEVELS\",\n    \"STATE_NAME\",\n    \"REMOTE_TYPE_NAME\"\n)\n\n# --- Handle numeric nulls ---\nnumeric_cols = [\"MIN_YEARS_EXPERIENCE\", \"MIN_EDULEVELS\"]\nfor c in numeric_cols:\n    model_df = model_df.withColumn(c, col(c).cast(DoubleType()))\n    mean_val = model_df.select(_mean(col(c))).first()[0]\n    model_df = model_df.na.fill({c: mean_val})\n\n# --- Handle categorical nulls ---\ncategorical_cols = [\"NAICS2_NAME\", \"STATE_NAME\", \"REMOTE_TYPE_NAME\"]\nmodel_df = model_df.filter(\n    (trim(col(\"STATE_NAME\")) != \"\") &\n    (trim(col(\"REMOTE_TYPE_NAME\")) != \"\") &\n    (trim(col(\"NAICS2_NAME\")) != \"\") &\n    col(\"STATE_NAME\").isNotNull() &\n    col(\"REMOTE_TYPE_NAME\").isNotNull() &\n    col(\"NAICS2_NAME\").isNotNull()\n)\n\n# --- Encode categorical variables ---\nindexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_IDX\", handleInvalid=\"keep\") for c in categorical_cols]\nencoders = [OneHotEncoder(inputCol=f\"{c}_IDX\", outputCol=f\"{c}_VEC\") for c in categorical_cols]\n\n# --- Assemble numeric features ---\nnumeric_assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"numeric_features\")\n\n# --- Scale numeric features to 0-1 ---\nscaler = MinMaxScaler(inputCol=\"numeric_features\", outputCol=\"numeric_scaled\")\n\n# --- Assemble final feature vector (scaled numeric + categorical vectors) ---\nfinal_assembler = VectorAssembler(\n    inputCols=[\"numeric_scaled\"] + [f\"{c}_VEC\" for c in categorical_cols],\n    outputCol=\"features\"\n)\n\n# --- Logistic Regression classifier ---\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"requires_ai\", maxIter=50, regParam=0.0, elasticNetParam=0.0)\n\n# --- Build pipeline ---\npipeline = Pipeline(stages=indexers + encoders + [numeric_assembler, scaler, final_assembler, lr])\n\n# --- Split train/test and fit ---\ntrain_df, test_df = model_df.randomSplit([0.8, 0.2], seed=42)\nlr_model = pipeline.fit(train_df)\n\nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n25/10/15 13:53:38 WARN Utils: Your hostname, Kellys-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.6 instead (on interface en0)\n25/10/15 13:53:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/10/15 13:53:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/10/15 13:53:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n[Stage 1:&gt;                                                          (0 + 1) / 1]                                                                                [Stage 2:&gt;                                                          (0 + 1) / 1]                                                                                [Stage 5:&gt;                                                          (0 + 1) / 1]                                                                                [Stage 8:&gt;                                                          (0 + 1) / 1]                                                                                [Stage 14:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 20:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 26:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 29:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 30:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n\n# --- Predict & evaluate ---\npredictions = lr_model.transform(test_df)\nevaluator = BinaryClassificationEvaluator(labelCol=\"requires_ai\", metricName=\"areaUnderROC\")\nroc_auc = evaluator.evaluate(predictions)\nprint(f\"ROC-AUC: {roc_auc:.3f}\")\n\n# --- Extract logistic regression stage ---\nlr_stage = lr_model.stages[-1]\n\n# LogisticRegressionModel in Spark stores coefficients in a dense vector\ncoefficients = lr_stage.coefficients.toArray()\n\n# --- Compute feature sizes ---\n# Number of numeric features\nnum_numeric = len(numeric_cols)\n# One-hot vector sizes for categorical features\nfeatures_vec = lr_model.transform(test_df).select(\"features\").first()[0]\nvector_size = len(features_vec)\nnum_categorical = len(categorical_cols)\ncategorical_vector_sizes = [(vector_size - num_numeric)//num_categorical]*num_categorical\n\n# --- Aggregate coefficients by feature ---\nfeature_sizes = [1]*num_numeric + categorical_vector_sizes\nfeature_names = numeric_cols + categorical_cols\n\nagg_importances = []\nstart = 0\nfor size in feature_sizes:\n    # Take mean of absolute values of coefficients for categorical features\n    agg_importances.append(np.mean(np.abs(coefficients[start:start+size])))\n    start += size\n\n# --- Create DataFrame & plot ---\nimportance_df = pd.DataFrame({\n    \"Feature\": feature_names,\n    \"Importance\": agg_importances\n}).sort_values(\"Importance\", ascending=False)\n\nplt.figure(figsize=(8,5))\nplt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\nplt.xlabel(\"Mean Absolute Coefficient\")\nplt.title(\"Logistic Regression Feature Importances (mean per categorical vector)\")\nplt.gca().invert_yaxis()\nplt.savefig(\"output/logistic_regression.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\n\n[Stage 45:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\nROC-AUC: 0.655\n\n\n[Stage 54:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n\n\n\n\n\n\n\nROC-AUC was used instead of accuracy because it is more suited for binary classifications such as AI vs non-AI.\n\n# --- Create sample job listings ---\nsample_jobs = [\n    Row(MIN_YEARS_EXPERIENCE=15.0, NAICS2_NAME=\"Professional, Scientific, and Technical Services\", STATE_NAME=\"New York\", MIN_EDULEVELS=3.0, REMOTE_TYPE_NAME=\"Remote\"),\n    Row(MIN_YEARS_EXPERIENCE=5.0, NAICS2_NAME=\"Finance and Insurance\", STATE_NAME=\"Texas\", MIN_EDULEVELS=2.0, REMOTE_TYPE_NAME=\"Remote\"),\n    Row(MIN_YEARS_EXPERIENCE=10.0, NAICS2_NAME=\"Real Estate and Rental and Leasing\", STATE_NAME=\"Minnesota\", MIN_EDULEVELS=0.0, REMOTE_TYPE_NAME=\"Hybrid Remote\"),\n    Row(MIN_YEARS_EXPERIENCE=15.0, NAICS2_NAME=\"Health Care and Social Assistance\", STATE_NAME=\"New York\", MIN_EDULEVELS=4.0, REMOTE_TYPE_NAME=\"Not Remote\"),\n]\n\nsample_df = spark.createDataFrame(sample_jobs)\n\n# Ensure numeric columns are DoubleType\nfor c in [\"MIN_YEARS_EXPERIENCE\", \"MIN_EDULEVELS\"]:\n    sample_df = sample_df.withColumn(c, col(c).cast(DoubleType()))\n\n# --- Transform samples through the trained pipeline ---\npredictions = lr_model.transform(sample_df)\n\n# --- Extract probability of AI class ---\npredictions_array = predictions.withColumn(\"prob_array\", vector_to_array(col(\"probability\")))\n\n# --- Show results ---\nresults = predictions_array.select(\n    \"MIN_YEARS_EXPERIENCE\",\n    \"NAICS2_NAME\",\n    \"STATE_NAME\",\n    \"MIN_EDULEVELS\",\n    \"REMOTE_TYPE_NAME\",\n    col(\"prob_array\")[1].alias(\"AI_prob\"),\n    \"prediction\"\n)\n\nresults.show(truncate=False)\n\n+--------------------+------------------------------------------------+----------+-------------+----------------+-------------------+----------+\n|MIN_YEARS_EXPERIENCE|NAICS2_NAME                                     |STATE_NAME|MIN_EDULEVELS|REMOTE_TYPE_NAME|AI_prob            |prediction|\n+--------------------+------------------------------------------------+----------+-------------+----------------+-------------------+----------+\n|15.0                |Professional, Scientific, and Technical Services|New York  |3.0          |Remote          |0.5524002949151843 |1.0       |\n|5.0                 |Finance and Insurance                           |Texas     |2.0          |Remote          |0.17876889392822903|0.0       |\n|10.0                |Real Estate and Rental and Leasing              |Minnesota |0.0          |Hybrid Remote   |0.18450482855782802|0.0       |\n|15.0                |Health Care and Social Assistance               |New York  |4.0          |Not Remote      |0.1876643082460473 |0.0       |\n+--------------------+------------------------------------------------+----------+-------------+----------------+-------------------+----------+\n\n\n\n\n1 Logistic Regression Summary\nThis analysis skimmed the job listings for keywords to distinguish AI from non-AI job postings and used a logistic regression model trained on five features: minimum years of experience, NAICS2 classification, state, minimum education level, and remote work type. These features were chosen because AI roles typically require more experience, are concentrated in certain industries over others, demand advanced degrees, and could be more prevalent in specific regions, while remote flexibility can influence applicant reach. The model revealed that years of experience was by far the most influential factor, followed by NAICS2, reflecting the expected patterns of AI job characteristics.\nIn order to create the model, we encoded the categorical variables and used MinMaxScaler to size the continuous variables to a 0-1 scale, which allowed the training model to weigh each variable equally. After obtaining the model data, we then charted the feature importance, making sure to take the aggregate coefficients to avoid over-reporting the importance of the encoded variables. The model reveals that years of experience is the most important feature when determining whether or not a role involves AI. However, the combination of all factors can lead to dramatically different results. For example, when using the model against a sample data set, it predicted that the 15-year remote listing in New York in âProfessional, Scientific, and Technical Servicesâ would be an AI job, but the 15-year on-site listing in New York in âHealth Care and Social Assistanceâ would not.\nIn order to refine its predictive capabilities, this model could benefit from some fine-tuning; right now it is evaluating probability and deeming any data with over 50% chance as involving AI, but if we were to get more granular with the data, the ideal threshold to use in this process may be closer to 35% or 40%. Additionally, given the ever-changing landscape of AI and the continued trend of AI becoming commonplace in many jobs, that threshold may continue to lower with time. This tool should be consistently re-evaluated to maintain relevancy so that users can derive the most value from its results."
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Loading Libraries and Data",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nraw_df = pd.read_csv(\"data/lightcast_job_postings.csv\")\n#raw_df.columns.tolist()\n\n/var/folders/7j/ct705g296ls7nrjh30h9pyg40000gn/T/ipykernel_59017/1289692877.py:5: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False."
  },
  {
    "objectID": "skill_gap_analysis.html#top-strengths-per-person",
    "href": "skill_gap_analysis.html#top-strengths-per-person",
    "title": "Loading Libraries and Data",
    "section": "2.1 Top strengths per person**",
    "text": "2.1 Top strengths per person**\n\ntop_strengths = df_skills.apply(lambda row: row[row == row.max()].index.tolist(), axis=1)\ntop_strengths.to_frame(name=\"Top Strength Skills\")\n\n\n\n\n\n\n\n\nTop Strength Skills\n\n\nName\n\n\n\n\n\nMackenzie\n[Detail Oriented, Presentations]\n\n\nSabrina\n[Tableau (Business Intelligence Software)]\n\n\nKelly\n[Excel]"
  },
  {
    "objectID": "skill_gap_analysis.html#team-averages-by-skill",
    "href": "skill_gap_analysis.html#team-averages-by-skill",
    "title": "Loading Libraries and Data",
    "section": "2.2 Team averages by skill",
    "text": "2.2 Team averages by skill\n\nteam_avg = df_skills.mean().sort_values(ascending=False)\nteam_avg.to_frame(name=\"Team Average (1â5)\")\n\n\n\n\n\n\n\n\nTeam Average (1â5)\n\n\n\n\nCommunication\n4.000000\n\n\nTableau (Business Intelligence Software)\n4.000000\n\n\nDetail Oriented\n4.000000\n\n\nProblem Solving\n3.666667\n\n\nLeadership\n3.666667\n\n\nExcel\n3.333333\n\n\nPresentations\n3.333333\n\n\nPlanning\n3.333333\n\n\nSQL (Programming Language)\n3.333333\n\n\nData Analysis\n3.000000\n\n\nDashboard\n3.000000\n\n\nManagement\n3.000000\n\n\nBusiness Process\n2.666667\n\n\nPython (Programming Language)\n2.666667\n\n\nSAP Applications\n2.666667\n\n\nProject Management\n2.000000\n\n\nFinance\n2.000000\n\n\nBusiness Requirements\n2.000000\n\n\nOperations\n1.666667\n\n\nComputer Science\n1.666667"
  },
  {
    "objectID": "skill_gap_analysis.html#extract-the-most-in-demand-skills-from-it-job-postings",
    "href": "skill_gap_analysis.html#extract-the-most-in-demand-skills-from-it-job-postings",
    "title": "Loading Libraries and Data",
    "section": "3.1 Extract the Most In-Demand Skills from IT Job Postings",
    "text": "3.1 Extract the Most In-Demand Skills from IT Job Postings\n\nfrom collections import Counter\nimport ast\nimport pandas as pd\n\n#  Parse SKILLS_NAME into Python lists\nraw_df[\"SKILLS_NAME\"] = raw_df[\"SKILLS_NAME\"].apply(\n    lambda x: ast.literal_eval(x) if isinstance(x, str) and x.strip().startswith(\"[\") else (x if isinstance(x, list) else [])\n)\n\n# rename things so similar ones match and stay consistent\n\nalias_map = {\n    \"SQL\": \"SQL (Programming Language)\",\n    \"Sql\": \"SQL (Programming Language)\",\n    \"MS Excel\": \"Excel\",\n    \"Microsoft Excel\": \"Excel\",\n    \"PowerBI\": \"Power BI\",\n}\ndef canon_skill(s: str) -&gt; str:\n    s = s.strip()\n    return alias_map.get(s, s)\n\n# 3) Combines and count\nall_skills = [canon_skill(s) for sublist in raw_df[\"SKILLS_NAME\"] for s in (sublist if isinstance(sublist, list) else []) if isinstance(s, str)]\nskill_counts = Counter([s for s in all_skills if s])\n\n# 4) Top Skills \ntop_skills = [skill for skill, _ in skill_counts.most_common(20)]\nprint(\"Top skills (dataset):\", top_skills)\n\nTop skills (dataset): ['Communication', 'Data Analysis', 'Management', 'SQL (Programming Language)', 'Problem Solving', 'Leadership', 'Computer Science', 'Operations', 'Project Management', 'Business Process', 'Business Requirements', 'Excel', 'Finance', 'Python (Programming Language)', 'Detail Oriented', 'SAP Applications', 'Dashboard', 'Presentations', 'Tableau (Business Intelligence Software)', 'Planning']"
  },
  {
    "objectID": "skill_gap_analysis.html#industry-expertise-demand-data-driven-15",
    "href": "skill_gap_analysis.html#industry-expertise-demand-data-driven-15",
    "title": "Loading Libraries and Data",
    "section": "3.2 Industry expertise demand (data-driven 1â5)",
    "text": "3.2 Industry expertise demand (data-driven 1â5)\nIn this section, We built a simple market target for each top skill using what employers write in job postings. First, we joined the job title and description so we could read the text. If a post listed MIN_YEARS_EXPERIENCE, we used it; if not, we pulled numbers like â3+ yearsâ from the text and mapped years to a 1â5 level (with a small bump so â3 yearsâ sits in the middle of a 3â5 range). Next, we read seniority words in the title (junior vs.Â senior/lead/manager) and skill phrases in the text (basic, intermediate, advanced, expert) and turned those into levels too. For each posting and skill, we combined the three signals, years (50%), seniority (30%), and phrases (20%) to get one score. Finally, we averaged those scores across all postings for each skill to get a 1â5 industry target, which we used to compare with our teamâs ratings.\n\nimport re\nimport numpy as np\n\n# Build text fields if they exist; otherwise empty strings\ntitle_col = \"TITLE\" if \"TITLE\" in raw_df.columns else None\nbody_col  = \"BODY\"  if \"BODY\"  in raw_df.columns else None\n\ntext_title = raw_df[title_col].astype(str).str.lower() if title_col else \"\"\ntext_body  = raw_df[body_col].astype(str).str.lower()  if body_col  else \"\"\ntext_all   = (text_title + \" \" + text_body).astype(str).str.strip()\n\n# Prefer MIN_YEARS_EXPERIENCE if present; else parse \"3+ years\" from text\nmin_col = \"MIN_YEARS_EXPERIENCE\" if \"MIN_YEARS_EXPERIENCE\" in raw_df.columns else None\nmin_years = pd.to_numeric(raw_df[min_col], errors=\"coerce\") if min_col else pd.Series(np.nan, index=raw_df.index)\n\ndef extract_years_from_text(text):\n    if not isinstance(text, str): return np.nan\n    m = re.search(r'(\\d+)\\s*\\+?\\s*(?:years?|yrs)\\s+(?:of\\s+)?experience', text, flags=re.I)\n    return float(m.group(1)) if m else np.nan\n\nyears_from_text = text_all.apply(extract_years_from_text)\nyears_req = min_years.where(min_years.notna(), years_from_text)\n\ndef years_to_level_from_min(y):\n    if pd.isna(y): return np.nan\n    y = float(y)\n    base = 1 if y &lt; 1 else 2 if y &lt; 2 else 3 if y &lt; 4 else 4 if y &lt; 6 else 5\n    return min(5, base + 0.3)  # small bump so \"3 years\" â mid of typical 3â5\n\nyears_level = years_req.apply(years_to_level_from_min)\n\ndef seniority_from_title(t):\n    if not isinstance(t, str): return np.nan\n    if re.search(r'\\b(intern|junior|jr|entry)\\b', t):             return 2\n    if re.search(r'\\b(senior|sr|lead|principal|architect)\\b', t): return 5\n    if re.search(r'\\b(manager|director|head)\\b', t):              return 5\n    return 3\n\nseniority_level = text_title.apply(seniority_from_title) if title_col else pd.Series(np.nan, index=raw_df.index)\n\nPHRASE_LEVELS = [\n    (r'\\b(expert|expertise|mastery|guru)\\b', 5),\n    (r'\\b(advanced|in-depth|strong|proficient|hands-on|solid)\\b', 4),\n    (r'\\b(intermediate|working knowledge)\\b', 3),\n    (r'\\b(basic|knowledge of|familiarity)\\b', 2),\n]\ndef phrase_level(text):\n    if not isinstance(text, str): return np.nan\n    lvl = np.nan\n    for pat, v in PHRASE_LEVELS:\n        if re.search(pat, text):\n            lvl = v if pd.isna(lvl) else max(lvl, v)\n    return lvl\n\nphrase_level_series = text_all.apply(phrase_level)\n\n# Explode one row per (posting, skill), keep only Top 20 skills\nexploded = pd.DataFrame({\n    \"SKILLS_LIST\": raw_df[\"SKILLS_NAME\"],\n    \"years_level\": years_level,\n    \"seniority_level\": seniority_level,\n    \"phrase_level\": phrase_level_series\n}).explode(\"SKILLS_LIST\")\n\nexploded[\"SKILL\"] = exploded[\"SKILLS_LIST\"].astype(str).apply(canon_skill)\nexploded = exploded[exploded[\"SKILL\"].isin(top_skills)]\n\n# Combine signals â expected level per row\nw_years, w_seniority, w_phrase = 0.5, 0.3, 0.2\ndef combine_levels(row):\n    vals, wts = [], []\n    if pd.notna(row[\"years_level\"]):     vals.append(row[\"years_level\"]);     wts.append(w_years)\n    if pd.notna(row[\"seniority_level\"]): vals.append(row[\"seniority_level\"]); wts.append(w_seniority)\n    if pd.notna(row[\"phrase_level\"]):    vals.append(row[\"phrase_level\"]);    wts.append(w_phrase)\n    if not vals: return 3.0\n    return float(np.average(vals, weights=wts))\n\nexploded[\"EXPECTED_LEVEL\"] = exploded.apply(combine_levels, axis=1)\n\n# Final per-skill target (1â5)\nexpected_per_skill = (\n    exploded.groupby(\"SKILL\")[\"EXPECTED_LEVEL\"]\n            .mean()\n            .clip(1,5)\n            .round(2)\n            .reindex(top_skills)\n)\nexpected_per_skill.name = \"Target (Data-Driven)\"\nexpected_per_skill\n\nSKILL\nCommunication                               3.73\nData Analysis                               3.60\nManagement                                  3.74\nSQL (Programming Language)                  3.67\nProblem Solving                             3.75\nLeadership                                  3.85\nComputer Science                            3.77\nOperations                                  3.73\nProject Management                          3.77\nBusiness Process                            3.84\nBusiness Requirements                       3.88\nExcel                                       3.60\nFinance                                     3.76\nPython (Programming Language)               3.66\nDetail Oriented                             3.61\nSAP Applications                            3.85\nDashboard                                   3.65\nPresentations                               3.76\nTableau (Business Intelligence Software)    3.66\nPlanning                                    3.80\nName: Target (Data-Driven), dtype: float64"
  },
  {
    "objectID": "skill_gap_analysis.html#team-skills-vs.-industry-requiments",
    "href": "skill_gap_analysis.html#team-skills-vs.-industry-requiments",
    "title": "Loading Libraries and Data",
    "section": "3.3 Team Skills Vs. Industry Requiments",
    "text": "3.3 Team Skills Vs. Industry Requiments\n\nimport pandas as pd\nimport numpy as np\n\n# --- 0) Preconditions: you already have ---\n# df_skills (index=Name, columns include your top_skills)\n# top_skills (list of skills to compare)\n# expected_per_skill (Series: index=skills, values=industry target 1â5)\n\n# --- 1) Ensure apples-to-apples (only top_skills) ---\ndf_team_top10 = (\n    df_skills.reindex(columns=top_skills, fill_value=0)\n             .apply(pd.to_numeric, errors=\"coerce\")\n             .fillna(0).clip(0,5)\n)\n\n# --- 2) Build tidy comparison table ---\nteam_avg = df_team_top10.mean(axis=0)\ntarget   = expected_per_skill.reindex(top_skills).astype(float)\ngap      = (target - team_avg)\n\n# Order columns: Skill, each personâ¦, Team Avg, Target, Gap\nmembers = df_team_top10.index.tolist()\nrows = []\nfor skill in top_skills:\n    row = {\n        \"Skill\": skill,\n        **{name: float(df_team_top10.loc[name, skill]) for name in members},\n        \"Team Avg\": round(float(team_avg[skill]), 2),\n        \"Target\":   round(float(target[skill]), 2) if pd.notna(target[skill]) else np.nan,\n        \"Gap (TargetâAvg)\": round(float(gap[skill]), 2) if pd.notna(gap[skill]) else np.nan,\n    }\n    rows.append(row)\n\ncomparison_table = pd.DataFrame(rows)\n\n# Sort by biggest gap (needs first)\ncomparison_table = comparison_table.sort_values(\"Gap (TargetâAvg)\", ascending=False, na_position=\"last\").reset_index(drop=True)\n\n# Nice rounding for member columns too\nfor name in members:\n    comparison_table[name] = comparison_table[name].round(2)\n\ncomparison_table\n\n\n\n\n\n\n\n\nSkill\nMackenzie\nSabrina\nKelly\nTeam Avg\nTarget\nGap (TargetâAvg)\n\n\n\n\n0\nComputer Science\n1.0\n3.0\n1.0\n1.67\n3.77\n2.10\n\n\n1\nOperations\n2.0\n1.0\n2.0\n1.67\n3.73\n2.06\n\n\n2\nBusiness Requirements\n3.0\n1.0\n2.0\n2.00\n3.88\n1.88\n\n\n3\nProject Management\n2.0\n2.0\n2.0\n2.00\n3.77\n1.77\n\n\n4\nFinance\n1.0\n2.0\n3.0\n2.00\n3.76\n1.76\n\n\n5\nSAP Applications\n4.0\n3.0\n1.0\n2.67\n3.85\n1.18\n\n\n6\nBusiness Process\n2.0\n3.0\n3.0\n2.67\n3.84\n1.17\n\n\n7\nPython (Programming Language)\n2.0\n3.0\n3.0\n2.67\n3.66\n0.99\n\n\n8\nManagement\n4.0\n2.0\n3.0\n3.00\n3.74\n0.74\n\n\n9\nDashboard\n4.0\n4.0\n1.0\n3.00\n3.65\n0.65\n\n\n10\nData Analysis\n2.0\n3.0\n4.0\n3.00\n3.60\n0.60\n\n\n11\nPlanning\n4.0\n3.0\n3.0\n3.33\n3.80\n0.47\n\n\n12\nPresentations\n5.0\n3.0\n2.0\n3.33\n3.76\n0.43\n\n\n13\nSQL (Programming Language)\n2.0\n4.0\n4.0\n3.33\n3.67\n0.34\n\n\n14\nExcel\n3.0\n2.0\n5.0\n3.33\n3.60\n0.27\n\n\n15\nLeadership\n4.0\n4.0\n3.0\n3.67\n3.85\n0.18\n\n\n16\nProblem Solving\n3.0\n4.0\n4.0\n3.67\n3.75\n0.08\n\n\n17\nCommunication\n4.0\n4.0\n4.0\n4.00\n3.73\n-0.27\n\n\n18\nTableau (Business Intelligence Software)\n4.0\n5.0\n3.0\n4.00\n3.66\n-0.34\n\n\n19\nDetail Oriented\n5.0\n4.0\n3.0\n4.00\n3.61\n-0.39\n\n\n\n\n\n\n\n\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Pick which columns to plot (only numbers!)\nmembers = df_team_top10.index.tolist()  \nnumeric_cols = members + [\"Team Avg\", \"Target\"]  \n\n#  Make a numeric matrix indexed by Skill\nheatmap_df = (\n    comparison_table\n      .set_index(\"Skill\")[numeric_cols]\n      .apply(pd.to_numeric, errors=\"coerce\")\n)\n\n# 3) Plot\nos.makedirs(\"output\", exist_ok=True)\nplt.figure(figsize=(12, max(6, 0.45*len(heatmap_df))))  # grow height if many skills\nsns.heatmap(\n    heatmap_df, annot=True, fmt=\".2f\",\n    cmap=\"YlGnBu\", linewidths=0.5, vmin=0, vmax=5\n)\nplt.title(\"Team vs. Industry Expertise Demand â Top Skills (0â5)\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.savefig(\"output/team_vs_industry_expertise_heatmap.png\", dpi=300)\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\nThe team vs.Â industry headmad compares our skills to the market targets (bottom row). We meet or beat the target in communication and Tableau and are close on problem solving, Excel, and detail-oriented work. We are below the target in computer science, operations, project management, business requirements/process, leadership, finance, Python, and SQL. Our focus should be to raise those areas to about 3.5 or 4 with short courses and practice. Overall, we explain and show insights well, but we need stronger basics and delivery skills to match the market."
  },
  {
    "objectID": "skill_gap_analysis.html#gaps-market-adjusted-priorities",
    "href": "skill_gap_analysis.html#gaps-market-adjusted-priorities",
    "title": "Loading Libraries and Data",
    "section": "3.4 Gaps + Market Adjusted Priorities",
    "text": "3.4 Gaps + Market Adjusted Priorities\n\n# Unweighted gap (Target â Team Avg)\nteam_avg = df_team_top10.mean()\ngap = expected_per_skill - team_avg\n\ngap_table = (\n    pd.DataFrame({\n        \"Avg Team Level\": team_avg.round(2),\n        \"Target (Data-Driven)\": expected_per_skill.round(2),\n        \"Gap (Target â Avg)\": gap.round(2)\n    })\n    .sort_values(\"Gap (Target â Avg)\", ascending=False)\n)\ngap_table\n\n\n\n\n\n\n\n\nAvg Team Level\nTarget (Data-Driven)\nGap (Target â Avg)\n\n\n\n\nComputer Science\n1.67\n3.77\n2.10\n\n\nOperations\n1.67\n3.73\n2.06\n\n\nBusiness Requirements\n2.00\n3.88\n1.88\n\n\nProject Management\n2.00\n3.77\n1.77\n\n\nFinance\n2.00\n3.76\n1.76\n\n\nSAP Applications\n2.67\n3.85\n1.18\n\n\nBusiness Process\n2.67\n3.84\n1.17\n\n\nPython (Programming Language)\n2.67\n3.66\n0.99\n\n\nManagement\n3.00\n3.74\n0.74\n\n\nDashboard\n3.00\n3.65\n0.65\n\n\nData Analysis\n3.00\n3.60\n0.60\n\n\nPlanning\n3.33\n3.80\n0.47\n\n\nPresentations\n3.33\n3.76\n0.43\n\n\nSQL (Programming Language)\n3.33\n3.67\n0.34\n\n\nExcel\n3.33\n3.60\n0.27\n\n\nLeadership\n3.67\n3.85\n0.18\n\n\nProblem Solving\n3.67\n3.75\n0.08\n\n\nCommunication\n4.00\n3.73\n-0.27\n\n\nTableau (Business Intelligence Software)\n4.00\n3.66\n-0.34\n\n\nDetail Oriented\n4.00\n3.61\n-0.39\n\n\n\n\n\n\n\nThe gaps table shows where we sit below the market target. Positive numbers mean we need to improve; negative numbers mean we already meet or beat the target. Our biggest gaps are in computer science, operations, business requirements, project management, and finance. Smaller gaps show up in SAP, business process, and Python. We already match or exceed the market in communication, Tableau, and being detail-oriented.\nTo set priorities, we adjust each gap by how common the skill is in job postings (market-adjusted priority). Using that, we should first focus on computer science and operations, then business requirements and project management, followed by finance/SAP. This plan lets us close the largest, most market-relevant gaps first while keeping our strengths sharp."
  },
  {
    "objectID": "research_introduction.html",
    "href": "research_introduction.html",
    "title": "AI vs.Â Non-AI Careers",
    "section": "",
    "text": "We live in a time when artificial intelligence (AI) is changing the way people work. Across industries, tasks that were once done by humans are being automated, while new opportunities are appearing that require different skills. In 2024, this topic is especially important because AI tools are no longer limited to technology companies, they are now widely used in healthcare, education, energy, retail, and many other sectors. Understanding how AI affects the job market helps prepare for shifts in employment, salary trends, and the creation of new professions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hiring is shifting toward AI-enabled roles across industries, but we donât have a clear view of where demand is growing and how that affects our job search. Without that, we risk aiming at slow areas and missing interviews and offers.\nThis report provides multiple solutions and insights based on the Lightcase Job Posting data as well as our recommendations for those who are currently job hunting and looking for way to refine their search to align with their skillset."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 Kelly Sheng\nKelly Sheng graduated in 2021 from New York University with a Bachelorâs degree in Music Business. Her interest in data analytics began when she was tasked with improving efficiencies at her influencer marketing role, at which point she discovered the endless amount of insights that are available when one knows how to work with data.\nKelly now works in performance marketing and is in her final semester at Boston University, preparing to graduate with a Masterâs degree in Applied Business Analytics. She hopes to use the learnings she has gained along the way to further improve the service she can provide to her clients.\n\n\n2 Sabrina Minaya Vasquez\nSabrina Minaya earned a Bachelorâs degree in Computer Science at Boston University. She grew up with her father who is obsessed with computers and their innovation. At the age of 19, Sabrinaâs father showed her how he was developing a sales program for a liquor store. She was fascinated by how he created a program that can search items, do the inventory of any item, calculate the total amount sold by the store, etc. The fact that Sabrinaâs father created something so amazing inspired her to go into technology.\nSheâs currently pursuing a Masterâs degree in Applied Business Analytics. Sabrina is gaining a number of skills through the program, such as statistical analysis, data mining, machine learning, data visualization, and quantitative and qualitative decision-making.\n\n\n3 Mackenzie Howard\nMackenzie have been working in the travel industry as a business development manager for the last 4.5 years. She create business cases, pricing analyses, travel policy structuring, and have extensive experience in forecasting. Mackenzie is hoping to leverage her Masterâs degree to help her move into a more strategic, problem-solving, and data-oriented account management position within aviation."
  },
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "Loading Libraries and Data",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nraw_df = pd.read_csv(\"data/lightcast_job_postings.csv\")\n#raw_df.columns.tolist()\n\n/var/folders/7j/ct705g296ls7nrjh30h9pyg40000gn/T/ipykernel_58932/1289692877.py:5: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n1 Cleaning Data\n\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\nraw_df.drop(columns=columns_to_drop, inplace=True)\n\n# Fill missing values\nraw_df[\"SALARY\"].fillna(raw_df[\"SALARY\"].median(), inplace=True)\nraw_df[\"NAICS_2022_6\"].fillna(\"Unknown\", inplace=True)\n\n# Drop columns with &gt;50% missing values\nraw_df.dropna(thresh=len(raw_df) * 0.5, axis=1, inplace=True)\n\nraw_df = raw_df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"], keep=\"first\")\n\n#raw_df.head()\n\n/var/folders/7j/ct705g296ls7nrjh30h9pyg40000gn/T/ipykernel_58932/2470702404.py:9: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n/var/folders/7j/ct705g296ls7nrjh30h9pyg40000gn/T/ipykernel_58932/2470702404.py:10: FutureWarning:\n\nA value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n\n/var/folders/7j/ct705g296ls7nrjh30h9pyg40000gn/T/ipykernel_58932/2470702404.py:10: FutureWarning:\n\nSetting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n\n\n\n\n\n2 Plot setup for AI vs Non-AI job posting count\n\nimport os, re\nimport pandas as pd\n\n# Kellyâs cleaned dataframe must exist\nassert \"raw_df\" in globals(), \"raw_df must exist (Kellyâs cleaned dataframe).\"\ndf = raw_df.copy()\n\n# Parse date -&gt; month\nif \"POSTED\" not in df.columns:\n    raise ValueError(\"Expected a POSTED column in raw_df.\")\ndf[\"POSTED_DT\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"POSTED_DT\"])\ndf[\"month\"] = df[\"POSTED_DT\"].dt.to_period(\"M\").dt.to_timestamp()\n\n# Combine likely text fields\ncandidate_text_cols = [\n    \"TITLE\",\"TITLE_CLEAN\",\"TITLE_NAME\",\"BODY\",\n    \"SKILLS\",\"SKILLS_NAME\",\n    \"SPECIALIZED_SKILLS\",\"SPECIALIZED_SKILLS_NAME\",\n    \"SOFTWARE_SKILLS\",\"SOFTWARE_SKILLS_NAME\",\n    \"COMMON_SKILLS\",\"COMMON_SKILLS_NAME\",\n    \"CERTIFICATIONS_NAME\",\n]\ntext_cols = [c for c in candidate_text_cols if c in df.columns]\nif not text_cols:\n    text_cols = [\"TITLE\"] if \"TITLE\" in df.columns else []\ntext_series = (\n    df[text_cols].astype(str).agg(\" \".join, axis=1).str.lower()\n    if text_cols else pd.Series([\"\"] * len(df), index=df.index)\n)\n\n# FIXED list (no cut-off strings)\nai_terms = [\n    \"artificial intelligence\",\"ai\",\"machine learning\",\"deep learning\",\n    \"neural network\",\"nlp\",\"natural language\",\"computer vision\",\n    \"reinforcement learning\",\"generative ai\",\"llm\",\"gpt\",\"chatgpt\",\n    \"transformer\",\"bert\",\"prompt engineer\",\"prompt engineering\"\n]\n\n# Word-boundary pattern so we don't match 'retail' for 'ai'\nai_pattern = re.compile(\n    r\"\\b(?:\"\n    + \"|\".join(re.escape(t) for t in ai_terms)\n    + r\")\\b\",\n    flags=re.IGNORECASE,\n)\n\ndf[\"IS_AI\"] = text_series.str.contains(ai_pattern, na=False)\n\n# Monthly counts\nmonthly = (\n    df.groupby([\"month\",\"IS_AI\"])\n      .size()\n      .unstack(fill_value=0)\n      .rename(columns={True: \"AI\", False: \"Non-AI\"})\n      .sort_index()\n)\n\nmonthly_plt = monthly.copy()\nmonthly_plt.head(12)\n\n\n\n\n\n\n\nIS_AI\nNon-AI\nAI\n\n\nmonth\n\n\n\n\n\n\n2024-05-01\n12460\n1503\n\n\n2024-06-01\n11462\n2263\n\n\n2024-07-01\n8463\n3720\n\n\n2024-08-01\n10619\n3977\n\n\n2024-09-01\n10148\n4582\n\n\n\n\n\n\n\n\n\n3 Job Postings for AI vs Non-AI Jobs\n\nimport os\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\n\nassert \"monthly_plt\" in globals(), \"Run the setup chunk first.\"\n\nos.makedirs(\"output\", exist_ok=True)\n\nplt.figure(figsize=(12, 6))\nplt.plot(monthly_plt.index, monthly_plt[\"AI\"],     marker=\"o\", linewidth=2, label=\"AI jobs\")\nplt.plot(monthly_plt.index, monthly_plt[\"Non-AI\"], marker=\"o\", linewidth=2, label=\"Non-AI jobs\")\nplt.title(\"AI vs Non-AI Job Postings Over Time (Monthly)\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Number of Job Postings\")\nplt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{int(x):,}\"))\nplt.grid(True, alpha=0.25)\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"output/ai_vs_nonai_over_time.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nThe AI VS Non-AI Jobs graph hows two clear trends:\n\nAI jobs have been increasing uninterruptedly each month from May to September 2024. This suggests growing demand for AI-related roles.\nNon-AI jobs started much higher but declined between May and July before slightly recovering in August. By September, they still remained lower than at the start.\n\nIn summary, there are still more Non-AI jobs than AI jobs in total; however, the number of available AI jobs is increasing rapidly. This transition indicates the shifting of the job market towards the AI-based roles.\n\n\n4 Prep for monthly AI counts\n\nimport os, re\nimport numpy as np\nimport pandas as pd\n\nassert \"raw_df\" in globals(), \"raw_df must exist.\"\n\ndf = raw_df.copy()\n\n# ---- Dates -&gt; month ----\ndf[\"POSTED_DT\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"POSTED_DT\"])\ndf[\"month\"] = df[\"POSTED_DT\"].dt.to_period(\"M\").dt.to_timestamp()\n\n# ---- AI detector (reuse if already present) ----\nif \"IS_AI\" not in df.columns:\n    ai_terms = [\n        \"artificial intelligence\",\"ai\",\"machine learning\",\"deep learning\",\n        \"neural network\",\"nlp\",\"natural language\",\"computer vision\",\n        \"reinforcement learning\",\"generative ai\",\"llm\",\"gpt\",\"chatgpt\",\n        \"transformer\",\"bert\",\"prompt engineer\",\"prompt engineering\"\n    ]\n    text_cols = [c for c in [\n        \"TITLE\",\"TITLE_CLEAN\",\"BODY\",\n        \"SKILLS\",\"SKILLS_NAME\",\n        \"SPECIALIZED_SKILLS\",\"SPECIALIZED_SKILLS_NAME\",\n        \"SOFTWARE_SKILLS\",\"SOFTWARE_SKILLS_NAME\",\n        \"COMMON_SKILLS\",\"COMMON_SKILLS_NAME\",\n        \"CERTIFICATIONS_NAME\"\n    ] if c in df.columns]\n    combined = (df[text_cols].astype(str).agg(\" \".join, axis=1).str.lower()\n                if text_cols else pd.Series([\"\"], index=df.index))\n    pattern = re.compile(r\"\\b(?:%s)\\b\" % \"|\".join(re.escape(t) for t in ai_terms), re.I)\n    df[\"IS_AI\"] = combined.str.contains(pattern, na=False)\n\n# ---- Pick the best available industry label ----\nind_candidates = [\n    \"NAICS_2022_6_NAME\",\"NAICS6_NAME\",\n    \"NAICS_2022_4_NAME\",\"NAICS4_NAME\",\n    \"NAICS_2022_2_NAME\",\"NAICS2_NAME\",\n    \"NAICS_2022_6\",\"NAICS6\"\n]\nIND_COL = next((c for c in ind_candidates if c in df.columns), None)\nif IND_COL is None:\n    raise ValueError(\"No NAICS/industry name/code columns found.\")\n\n# ---- Monthly AI counts per industry ----\nai = df[df[\"IS_AI\"]].copy()\nai_monthly = (\n    ai.groupby([IND_COL, \"month\"])\n      .size()\n      .reset_index(name=\"count\")\n)\n\n# ---- Compute growth: (last 3-mo avg - first 3-mo avg) / first 3-mo avg ----\ndef growth_row(g):\n    g = g.sort_values(\"month\")\n    k = min(3, len(g))\n    first = g[\"count\"].iloc[:k].mean()\n    last  = g[\"count\"].iloc[-k:].mean()\n    total = g[\"count\"].sum()\n    if k &lt; 2 or first == 0:\n        return pd.Series({\"growth_pct\": np.nan, \"first_avg\": first, \"last_avg\": last, \"months\": len(g), \"total\": total})\n    return pd.Series({\"growth_pct\": (last - first) / first * 100.0,\n                      \"first_avg\": first, \"last_avg\": last,\n                      \"months\": len(g), \"total\": total})\n\ngrowth_df = ai_monthly.groupby(IND_COL).apply(growth_row).reset_index()\n\n# filter out tiny-volume industries to avoid wild % changes\nMIN_TOTAL = 30\ngrowth_df = growth_df[growth_df[\"total\"] &gt;= MIN_TOTAL].dropna(subset=[\"growth_pct\"])\n\n# pick top movers (adjust top_n)\ntop_n = 12\ngrowth_top = growth_df.sort_values(\"growth_pct\", ascending=False).head(top_n)\n\n# Save a copy if want a table in the doc\ngrowth_top_rounded = growth_top.copy()\ngrowth_top_rounded[\"growth_pct\"] = growth_top_rounded[\"growth_pct\"].round(1)\ngrowth_top_rounded.head(top_n)\n\n/var/folders/7j/ct705g296ls7nrjh30h9pyg40000gn/T/ipykernel_58932/672499099.py:67: DeprecationWarning:\n\nDataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n\n\n\n\n\n\n\n\n\n\nNAICS_2022_6_NAME\ngrowth_pct\nfirst_avg\nlast_avg\nmonths\ntotal\n\n\n\n\n260\nOffices of Other Holding Companies\n372.7\n3.666667\n17.333333\n5.0\n56.0\n\n\n300\nOther Motor Vehicle Parts Manufacturing\n330.8\n4.333333\n18.666667\n5.0\n68.0\n\n\n358\nResearch and Development in the Physical, Engi...\n176.2\n14.000000\n38.666667\n5.0\n133.0\n\n\n137\nExecutive Search Services\n160.0\n6.666667\n17.333333\n5.0\n59.0\n\n\n80\nComputer and Computer Peripheral Equipment and...\n154.5\n3.666667\n9.333333\n5.0\n33.0\n\n\n406\nTemporary Help Services\n148.8\n42.333333\n105.333333\n5.0\n352.0\n\n\n189\nHuman Resources Consulting Services\n125.6\n13.000000\n29.333333\n5.0\n96.0\n\n\n296\nOther Management Consulting Services\n116.4\n46.666667\n101.000000\n5.0\n337.0\n\n\n427\nWholesale Trade Agents and Brokers\n109.5\n7.000000\n14.666667\n5.0\n52.0\n\n\n325\nPharmaceutical Preparation Manufacturing\n108.0\n8.333333\n17.333333\n5.0\n62.0\n\n\n280\nOther Computer Related Services\n103.8\n70.666667\n144.000000\n5.0\n509.0\n\n\n415\nUnclassified Industry\n101.8\n280.666667\n566.333333\n5.0\n2025.0\n\n\n\n\n\n\n\nIn this table, we can see AI job postings growing across many industries. A few sectors jump fast but are small, like Other Holding Companies and Motor Vehicle Parts, and sound signals, but the totals are low, so that they may swing. The strongest, faster growth in real volume is in Temporary Help Services, Other Management Consulting, and Other Computer-Related Services; their monthly averages more than doubled. R&D and Executive Search are up too, with mid totals. There is also a growing group called âUnclassified.â This group includes various categories, so we need to use this data carefully.\n\n\n5 AI-Driven Job Growth by Industry\n\n# --- Short, readable labels + plot & save (Option B) -------------------------\nimport os, re\nfrom textwrap import shorten\nimport matplotlib.pyplot as plt\n\n# Use existing industry column if defined; otherwise default:\nIND_COL = IND_COL if \"IND_COL\" in globals() else \"NAICS_2022_6_NAME\"\n\ndef short_label(s: str) -&gt; str:\n    s = str(s)\n    # Common abbreviations to keep labels compact but clear\n    s = re.sub(r'(?i)\\bservices?\\b', 'Svcs', s)\n    s = re.sub(r'(?i)\\bmanufacturing\\b', 'Mfg', s)\n    s = re.sub(r'(?i)\\bpreparation\\b', 'Prep', s)\n    s = re.sub(r'(?i)\\bmanagement\\b', 'Mgmt', s)\n    s = re.sub(r'(?i)\\bcomputer\\b', 'Comp', s)\n    s = re.sub(r'(?i)\\bequipment\\b', 'Equip', s)\n    s = re.sub(r'(?i)\\bwholesale\\b', 'Whsl', s)\n    s = re.sub(r'(?i)\\bagents?\\b', 'Agts', s)\n    s = re.sub(r'(?i)\\bscientific\\b', 'Sci', s)\n    s = re.sub(r'(?i)\\bengineering\\b', 'Eng', s)\n    s = re.sub(r'(?i)\\band\\b', '&', s)\n    s = re.sub(r'(?i)\\bother\\b\\s*', '', s)  # drop leading \"Other\"\n    # Final safe trim\n    return shorten(s.strip(), width=32, placeholder='â¦')\n\nplot_df = growth_top.copy()\nplot_df[\"label\"] = plot_df[IND_COL].astype(str).map(short_label)\n\nfig, ax = plt.subplots(figsize=(11, 7))\nbars = ax.barh(plot_df[\"label\"], plot_df[\"growth_pct\"], color=\"#2F6DB3\", edgecolor=\"black\")\nax.invert_yaxis()  # biggest at top\n\nax.set_title(\"AI Job Posting Growth by Industry (First vs Last Month)\", fontsize=16)\nax.set_xlabel(\"Growth (%)\", fontsize=12)\nax.set_ylabel(\"Industry\", fontsize=12)\nax.grid(axis=\"x\", alpha=0.25)\n\n# Value labels\nfor b in bars:\n    w = b.get_width()\n    ax.text(\n        w + (1 if w &gt;= 0 else -1),\n        b.get_y() + b.get_height() / 2,\n        f\"{w:.1f}%\",\n        va=\"center\",\n        ha=\"left\" if w &gt;= 0 else \"right\",\n        fontsize=10,\n    )\n\nplt.tight_layout()\nos.makedirs(\"output\", exist_ok=True)\nplt.savefig(\"output/ai_industry_growth_shortlabels.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nThe chart shows that AI job postings are increasing quickly in many industries. The largest increases are in Offices of Holding Companies (about 373%) and Motor Vehicle Parts Manufacturing (about 331%). We also see significant growth in R&D (physical/engineering) (about 176%), Executive Search (about 160%), Computer & Peripheral Equipment (about 155%), and Temporary Help Services (about 149%). Other sectors like HR Consulting, Management Consulting, Wholesale Trade Agents, Pharmaceutical Preparation, Other Computer Related, and Unclassified categories enlarged by slightly more than 100% all together. The growth is mainly distributed among the different sectors, while only some industries exhibit the highest increases.\n\n\n6 Non-AI Job Posting Growth by Industry\n\nimport os, re\nimport pandas as pd\nfrom textwrap import shorten\n\nassert \"raw_df\" in globals(), \"raw_df must exist (cleaned dataframe).\"\ndf = raw_df.copy()\n\n# Pick an industry-name column that exists\nIND_COL = next((c for c in [\n    \"NAICS_2022_6_NAME\",\"NAICS6_NAME\",\"NAICS_2022_4_NAME\",\"NAICS4_NAME\",\n    \"NAICS_2022_2_NAME\",\"NAICS2_NAME\"\n] if c in df.columns), None)\nassert IND_COL, \"No industry/NAICS name column found.\"\n\nTITLE_COL = \"TITLE_CLEAN\" if \"TITLE_CLEAN\" in df.columns else \"TITLE\"\nassert TITLE_COL in df.columns, \"Need a TITLE or TITLE_CLEAN column.\"\n\n# Dates â month\ndf[\"POSTED_DT\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"POSTED_DT\", IND_COL])\ndf[\"month\"] = df[\"POSTED_DT\"].dt.to_period(\"M\").dt.to_timestamp()\n\n# Detect AI vs non-AI via title keywords\nAI_TERMS = [\n    \"artificial intelligence\", r\"\\bAI\\b\", \"machine learning\", r\"\\bML\\b\",\n    \"deep learning\", r\"\\bLLM\\b\", r\"\\bNLP\\b\", \"computer vision\",\n    \"generative\", \"chatgpt\", r\"gpt-\\d+\", \"transformer\", \"bert\",\n    \"prompt engineer\", \"reinforcement learning\"\n]\nai_pat = re.compile(\"|\".join(AI_TERMS), flags=re.IGNORECASE)\nif \"is_ai\" not in df.columns:\n    df[\"is_ai\"] = df[TITLE_COL].astype(str).str.contains(ai_pat, na=False)\n\n# Use industries from  AI-growth visual if available; otherwise pick top 10 by AI postings last month\nif \"growth_top\" in globals():\n    target_inds = [str(x) for x in growth_top[IND_COL].dropna().tolist()]\nelse:\n    last_m = df[\"month\"].max()\n    ai_last = (df[df[\"is_ai\"] & (df[\"month\"] == last_m)]\n               .groupby(IND_COL).size().sort_values(ascending=False).head(10))\n    target_inds = ai_last.index.astype(str).tolist()\n\n# Build non-AI growth (% firstâlast month) for those industries\nnon_ai = df[(~df[\"is_ai\"]) & (df[IND_COL].astype(str).isin(target_inds))].copy()\nmonthly = (non_ai.groupby([IND_COL, \"month\"]).size().reset_index(name=\"count\"))\n\nfirst_last = (monthly.sort_values(\"month\")\n                     .groupby(IND_COL, as_index=False)\n                     .agg(first=(\"count\",\"first\"), last=(\"count\",\"last\")))\nfirst_last = first_last[first_last[\"first\"] &gt; 0].copy()\nfirst_last[\"growth_pct\"] = ((first_last[\"last\"] - first_last[\"first\"])\n                            / first_last[\"first\"]) * 100.0\nfirst_last = first_last[first_last[IND_COL].astype(str).isin(target_inds)]\n\ndef short_label(s: str) -&gt; str:\n    s = str(s)\n    s = re.sub(r'(?i)\\bservices?\\b', 'Svcs', s)\n    s = re.sub(r'(?i)\\bmanufacturing\\b', 'Mfg', s)\n    s = re.sub(r'(?i)\\bmanagement\\b', 'Mgmt', s)\n    s = re.sub(r'(?i)\\bpreparation\\b', 'Prep', s)\n    s = re.sub(r'(?i)\\bcomputer\\b', 'Comp', s)\n    s = re.sub(r'(?i)\\bequipment\\b', 'Equip', s)\n    s = re.sub(r'(?i)\\bwholesale\\b', 'Whsl', s)\n    s = re.sub(r'(?i)\\band\\b', '&', s)\n    s = re.sub(r'\\s+', ' ', s).strip()\n    return shorten(s, width=32, placeholder='â¦')\n\nnon_ai_growth_plot_df = (first_last\n                         .sort_values(\"growth_pct\", ascending=False)\n                         .assign(label=lambda d: d[IND_COL].map(short_label)))\n\nos.makedirs(\"output\", exist_ok=True)\nnon_ai_growth_plot_df.head(10)  # preview table\n\n\n\n\n\n\n\n\nNAICS_2022_6_NAME\nfirst\nlast\ngrowth_pct\nlabel\n\n\n\n\n8\nResearch and Development in the Physical, Engi...\n45\n114\n153.333333\nResearch & Development in theâ¦\n\n\n6\nOther Motor Vehicle Parts Manufacturing\n12\n22\n83.333333\nOther Motor Vehicle Parts Mfg\n\n\n1\nExecutive Search Services\n37\n58\n56.756757\nExecutive Search Svcs\n\n\n0\nComputer and Computer Peripheral Equipment and...\n16\n22\n37.500000\nComp & Comp Peripheral Equip &â¦\n\n\n5\nOther Management Consulting Services\n162\n195\n20.370370\nOther Mgmt Consulting Svcs\n\n\n3\nOffices of Other Holding Companies\n23\n25\n8.695652\nOffices of Other Holdingâ¦\n\n\n9\nTemporary Help Services\n272\n288\n5.882353\nTemporary Help Svcs\n\n\n10\nUnclassified Industry\n2028\n2025\n-0.147929\nUnclassified Industry\n\n\n2\nHuman Resources Consulting Services\n32\n31\n-3.125000\nHuman Resources Consulting Svcs\n\n\n7\nPharmaceutical Preparation Manufacturing\n61\n58\n-4.918033\nPharmaceutical Prep Mfg\n\n\n\n\n\n\n\n\nimport os\nimport matplotlib.pyplot as plt\n\n# If prep didn't run in this kernel, rebuild quickly\nif \"non_ai_growth_plot_df\" not in globals():\n    # Re-run a minimal rebuild using the same logic\n    assert \"raw_df\" in globals(), \"raw_df is required to rebuild plot data.\"\n    #  execute the prep cell above; keeping this here for resilience:\n    # (We simply import the name if it exists; otherwise advise to run prep.)\n    raise AssertionError(\"Run the prep chunk first to create non_ai_growth_plot_df.\")\n\ndfp = non_ai_growth_plot_df.copy()\n\nif dfp.empty:\n    print(\"non_ai_growth_plot_df is emptyânothing to plot. \"\n          \"Check that selected industries have non-AI rows in the first/last month.\")\nelse:\n    os.makedirs(\"output\", exist_ok=True)\n    colors = dfp[\"growth_pct\"].ge(0).map({True: \"#1f77b4\", False: \"#d62728\"}).to_numpy()\n\n    fig, ax = plt.subplots(figsize=(11, 7))\n    bars = ax.barh(dfp[\"label\"], dfp[\"growth_pct\"], color=colors, edgecolor=\"black\")\n\n    ax.axvline(0, color=\"black\", linewidth=1, alpha=0.6)\n    ax.invert_yaxis()\n    ax.set_title(\"Non-AI Job Posting Growth by Industry (First â Last Month)\", fontsize=16)\n    ax.set_xlabel(\"Growth (%)\")\n    ax.set_ylabel(\"Industry\")\n    ax.grid(axis=\"x\", alpha=0.25)\n\n    for b, v in zip(bars, dfp[\"growth_pct\"]):\n        ax.text(\n            v + (1 if v &gt;= 0 else -1),\n            b.get_y() + b.get_height() / 2,\n            f\"{v:.1f}%\",\n            va=\"center\",\n            ha=\"left\" if v &gt;= 0 else \"right\",\n            fontsize=10,\n        )\n\n    plt.tight_layout()\n    plt.savefig(\"output/non_ai_growth_pct_in_ai_growth_industries_colored.png\",\n                dpi=200, bbox_inches=\"tight\")\n    plt.show()\n\n\n\n\n\n\n\n\nThe Non-AI job posting growth chart shows the following:\n\nThe biggest gains are in Research & Development (~153%) and Motor Vehicle Parts (~83%). Executive Search (~57%) and Computer/Peripheral Equipment (~38%) also grew well. Management Consulting (~20%) is up, while Offices of Holding Companies (~9%) and Temporary Help (~6%) show small growth.\nA few areas are slipping: Wholesale Trade Agents & Brokers (~-23%), Other Computer-Related Services (~-15%), Pharmaceutical Prep (~-5%), and HR Consulting (~-3%) declined; Unclassified is flat.\n\nThis means for us that non-AI roles are still growing, just not as fast as AI roles. The non-AI opportunities we want more of should be targeted to R&D labs, automotive suppliers, executive search firms, and hardware/peripherals. Outreach to these fields will guarantee more non-AI opportunities. The areas of wholesale, general computer, and pharma-related services, HR consulting would be where one could select with more discrimination since the demand for these practices is on the decline.\n\n\n7 AI-powered roles vs non-ai roles salary prep\n\nimport os, re\nimport numpy as np\nimport pandas as pd\n\nassert \"raw_df\" in globals(), \"raw_df (cleaned dataframe) must exist.\"\n\ndf = raw_df.copy()\n\n# --- Pick columns robustly ---\nTITLE_COL = \"TITLE_CLEAN\" if \"TITLE_CLEAN\" in df.columns else \"TITLE\"\nassert TITLE_COL in df.columns, \"Need a TITLE or TITLE_CLEAN column.\"\n\nIND_COL = next((c for c in [\n    \"NAICS_2022_6_NAME\",\"NAICS6_NAME\",\"NAICS_2022_4_NAME\",\"NAICS4_NAME\",\n    \"NAICS_2022_2_NAME\",\"NAICS2_NAME\",\"LIGHTCAST_SECTORS_NAME\"\n] if c in df.columns), None)\nassert IND_COL, \"No industry/NAICS name column found.\"\n\n# Salary: prefer SALARY; otherwise average of FROM/TO if present\nsal = pd.to_numeric(df.get(\"SALARY\", np.nan), errors=\"coerce\")\nif sal.isna().all() and {\"SALARY_FROM\",\"SALARY_TO\"} &lt;= set(df.columns):\n    s_from = pd.to_numeric(df[\"SALARY_FROM\"], errors=\"coerce\")\n    s_to   = pd.to_numeric(df[\"SALARY_TO\"],   errors=\"coerce\")\n    sal = (s_from + s_to) / 2\ndf[\"salary_num\"] = pd.to_numeric(sal, errors=\"coerce\")\n\n# keep positive salaries only\ndf = df[df[\"salary_num\"] &gt; 0].copy()\n\n# Optional: cap extreme outliers so a few posts don't dominate the mean\nq_low, q_hi = df[\"salary_num\"].quantile([0.01, 0.99])\ndf[\"salary_num\"] = df[\"salary_num\"].clip(q_low, q_hi)\n\n# --- Tag AI vs non-AI using title keywords ---\nAI_TERMS = [\n    \"artificial intelligence\", r\"\\bAI\\b\", \"machine learning\", r\"\\bML\\b\",\n    \"deep learning\", r\"\\bLLM\\b\", r\"\\bNLP\\b\", \"computer vision\",\n    \"generative\", \"chatgpt\", r\"gpt-\\d+\", \"transformer\", r\"\\bbert\\b\",\n    \"prompt engineer\", \"reinforcement learning\", \"data scientist\", \"ai engineer\"\n]\nai_pat = re.compile(\"|\".join(AI_TERMS), flags=re.IGNORECASE)\ndf[\"is_ai\"] = df[TITLE_COL].astype(str).str.contains(ai_pat, na=False)\n\n# --- Find top industries by AI posting count (choose N)\nN = 10\nai_counts = (df[df[\"is_ai\"]]\n             .groupby(IND_COL, dropna=True)\n             .size()\n             .sort_values(ascending=False)\n             .head(N)\n             .rename(\"ai_posts\")\n             .reset_index())\ntop_inds = ai_counts[IND_COL].astype(str).tolist()\n\n# --- Compute average salary (AI vs non-AI) for those industries\nsubset = df[df[IND_COL].astype(str).isin(top_inds)].copy()\ngrp = subset.groupby([IND_COL, \"is_ai\"])[\"salary_num\"].agg([\"mean\", \"count\"]).reset_index()\n\nai_part  = grp[grp[\"is_ai\"]].rename(columns={\"mean\":\"avg_ai_salary\", \"count\":\"ai_postings\"})\nnon_part = grp[~grp[\"is_ai\"]].rename(columns={\"mean\":\"avg_nonai_salary\", \"count\":\"non_ai_postings\"})\n\nai_vs_trad_industry_salary = (\n    ai_counts[[IND_COL, \"ai_posts\"]]        # preserves AI-based ordering\n    .merge(ai_part[[IND_COL, \"avg_ai_salary\", \"ai_postings\"]], on=IND_COL, how=\"left\")\n    .merge(non_part[[IND_COL, \"avg_nonai_salary\", \"non_ai_postings\"]], on=IND_COL, how=\"left\")\n)\n\n# Clean up & preview\nai_vs_trad_industry_salary = ai_vs_trad_industry_salary.fillna(0)\nos.makedirs(\"output\", exist_ok=True)\nai_vs_trad_industry_salary.head(N)\n\n\n\n\n\n\n\n\nNAICS_2022_6_NAME\nai_posts\navg_ai_salary\nai_postings\navg_nonai_salary\nnon_ai_postings\n\n\n\n\n0\nAll Other Miscellaneous Food Manufacturing\n124\n125106.451613\n124\n108667.836735\n49\n\n\n1\nComputer Systems Design Services\n81\n120464.666667\n81\n123184.301252\n4073\n\n\n2\nEmployment Placement Agencies\n72\n117815.555556\n72\n113490.867107\n4244\n\n\n3\nDirect Health and Medical Insurance Carriers\n67\n102811.940299\n67\n116431.746753\n1386\n\n\n4\nUnclassified Industry\n57\n129006.070175\n57\n113253.334609\n9148\n\n\n5\nOther Computer Related Services\n53\n117909.245283\n53\n118771.586707\n1309\n\n\n6\nAdministrative Management and General Manageme...\n52\n156290.384615\n52\n133606.945132\n4447\n\n\n7\nOffices of Certified Public Accountants\n48\n116174.354167\n48\n130229.120365\n1645\n\n\n8\nSoftware Publishers\n35\n95534.285714\n35\n131028.541624\n973\n\n\n9\nCommercial Banking\n34\n145390.588235\n34\n120671.148515\n1919\n\n\n\n\n\n\n\n\nimport os, re\nimport matplotlib.pyplot as plt\nfrom textwrap import shorten\nfrom matplotlib.ticker import FuncFormatter\n\nassert \"ai_vs_trad_industry_salary\" in globals(), \"Run the prep chunk first.\"\n\ndfp = ai_vs_trad_industry_salary.copy()\nif dfp.empty:\n    print(\"No data to plot after filtering. Check AI detection or industry column.\")\nelse:\n    # shorten very long industry labels\n    def short_label(s: str) -&gt; str:\n        s = str(s)\n        s = re.sub(r'(?i)\\bservices?\\b', 'Svcs', s)\n        s = re.sub(r'(?i)\\bmanufacturing\\b', 'Mfg', s)\n        s = re.sub(r'(?i)\\bmanagement\\b', 'Mgmt', s)\n        s = re.sub(r'(?i)\\bcomputer\\b', 'Comp', s)\n        s = re.sub(r'(?i)\\bequipment\\b', 'Equip', s)\n        s = re.sub(r'\\s+', ' ', s).strip()\n        return shorten(s, width=36, placeholder='â¦')\n\n    dfp[\"label\"] = dfp[IND_COL].map(short_label)\n\n    x = range(len(dfp))\n    w = 0.38\n\n    fig, ax = plt.subplots(figsize=(14, 7))\n    b1 = ax.bar([i - w/2 for i in x], dfp[\"avg_ai_salary\"],    width=w, label=\"AI job posts\",     edgecolor=\"black\")\n    b2 = ax.bar([i + w/2 for i in x], dfp[\"avg_nonai_salary\"], width=w, label=\"Non-AI job posts\", edgecolor=\"black\")\n\n    ax.set_title(\"Average Salary â AI vs. Non-AI Posts in Top AI Industries\", fontsize=16)\n    ax.set_xlabel(\"Industry (Top by AI Posting Volume)\")\n    ax.set_ylabel(\"Average Salary ($)\")\n    ax.set_xticks(list(x))\n    ax.set_xticklabels(dfp[\"label\"], rotation=35, ha=\"right\")\n    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, pos: f\"${int(v):,}\"))\n    ax.grid(axis=\"y\", alpha=0.25)\n    ax.legend()\n\n    # annotate bars\n    for bars in (b1, b2):\n        for p in bars:\n            h = p.get_height()\n            if h &gt; 0:\n                ax.text(p.get_x()+p.get_width()/2, h, f\"${int(h):,}\",\n                        ha=\"center\", va=\"bottom\", fontsize=9)\n\n    plt.tight_layout()\n    os.makedirs(\"output\", exist_ok=True)\n    plt.savefig(\"output/ai_vs_nonai_avg_salary_by_industry.png\", dpi=200, bbox_inches=\"tight\")\n    plt.show()\n\n\n\n\n\n\n\n\nIn this graph, we compared pay for AI and non-AI roles across industries. In most of these sectors, AI jobs pay more; significant gaps show up in Administrative Management & General Services, Commercial Banking, Unclassified, and Food Manufacturing. A few places flip the pattern: Software Publishers, Offices of Certified Public Accountants, Health/Medical Insurance, and Computer Systems Design show higher pay for non-AI roles (or pay that is almost the same in Other Computer-Related Services). For our search, we should target AI roles in industries with clear premium pay, and be careful in the few sectors where AI pay trails non-AI; those roles may be more junior or support.\n\n\n8 Market Takeaways from our EDA\nAI does not affect every field the same. From our EDA, the fields that use AI consulting, computer services, R&D, and parts of banking show rising postings and often higher pay, so jobs look safer there. In contrast, areas with flat or falling non-AI postings, wholesale trade agents, some general computer services, pharma prep, and HR consulting look less secure unless they adopt AI tools. Routine, repeat tasks are most at risk, while roles that mix tech and people skills (analysis, client work, project work) look safer.\nLooking across industries, we see growth where AI is being put to work: other computer-related services, management consulting, temporary help services, R&D, and commercial banking. Meanwhile, we see possible displacement or slowdown on the non-AI side in wholesale trade agents, parts of computer services, pharma prep, and HR consulting. In short, sectors leaning into AI grow; those that donât may shrink or re-scope roles.\nCompared with traditional paths like mechanical engineering, farming, and retail, AI-powered roles are growing faster and usually pay more, according to our charts. Traditional fields still matter, but many tasks there now use AI (e.g., predictive maintenance in mechanical, precision ag in farming, demand forecasting in retail). People who add data and AI skills to their core trade tend to get better options and higher pay.\nFinally, we see new job titles emerging: AI/ML Engineer, MLOps/AI Ops, AI Product Manager, Prompt Engineer, Data/ML Analyst, AI Solutions Consultant, and AI Ethics/Governance, along with more analytics-heavy PM and consulting roles. Overall, where AI is adopted, jobs shift toward analysis, tools, and delivery, and hiring grows. Where AI is ignored, roles risk shrinking. Our best move is to build AI and delivery skills (CS basics, operations, project management) and target the growing sectors."
  },
  {
    "objectID": "KMeans_clustering_methods.html",
    "href": "KMeans_clustering_methods.html",
    "title": "Kmeans Clustering Setup",
    "section": "",
    "text": "import os, re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nos.makedirs(\"output\", exist_ok=True)\n\n# Use your cleaned frame if present; else load CSV\ntry:\n    df = raw_df.copy()\nexcept NameError:\n    df = pd.read_csv(\"data/lightcast_job_postings.csv\")\n\n# Pick a title/text column robustly\nfor c in [\"TITLE_CLEAN\", \"TITLE\", \"TITLE_NAME\", \"TITLE_RAW\"]:\n    if c in df.columns:\n        text_col = c\n        break\nelse:\n    raise ValueError(\"No title column found (TITLE_CLEAN/TITLE/TITLE_NAME/TITLE_RAW).\")\n\n# Coerce useful numerics if present\nfor c in [\"SALARY\", \"SALARY_FROM\", \"SALARY_TO\", \"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\", \"DURATION\"]:\n    if c in df.columns:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n# Candidate categoricals (kept if present & not too wide)\ncandidate_cat = [\n    \"REMOTE_TYPE_NAME\", \"STATE_NAME\", \"EMPLOYMENT_TYPE_NAME\",\n    \"COMPANY_IS_STAFFING\", \"NAICS_2022_6_NAME\", \"ONET_NAME\", \"SOC_2021_5_NAME\"\n]\ncat_cols = [c for c in candidate_cat if c in df.columns]\ncat_cols = [c for c in cat_cols if df[c].nunique(dropna=True) &lt;= 200]\n\nnum_cols = [c for c in [\"SALARY\", \"MIN_YEARS_EXPERIENCE\", \"MAX_YEARS_EXPERIENCE\", \"DURATION\"] if c in df.columns]\n\nprint(\"Using columns:\")\nprint(\"  text_col:\", text_col)\nprint(\"  cat_cols:\", cat_cols)\nprint(\"  num_cols:\", num_cols)\n\nRANDOM_STATE = 42\n\nUsing columns:\n  text_col: TITLE_CLEAN\n  cat_cols: ['REMOTE_TYPE_NAME', 'STATE_NAME', 'EMPLOYMENT_TYPE_NAME', 'COMPANY_IS_STAFFING', 'ONET_NAME', 'SOC_2021_5_NAME']\n  num_cols: ['SALARY', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'DURATION']\n\n\n/var/folders/7j/ct705g296ls7nrjh30h9pyg40000gn/T/ipykernel_59075/3257818786.py:20: DtypeWarning:\n\nColumns (19,30) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n1 Helpers\n\ndef _clean_text_input(x):\n    \"\"\"\n    Accept Series, 1-col DataFrame, or numpy array from ColumnTransformer\n    and return a plain Python list[str] with NaNs -&gt; \"\".\n    \"\"\"\n    if isinstance(x, pd.Series):\n        s = x\n    elif isinstance(x, pd.DataFrame):\n        s = x.iloc[:, 0]\n    elif isinstance(x, np.ndarray):\n        s = pd.Series(x.ravel())\n    else:\n        s = pd.Series(x)\n    s = s.astype(\"string\").fillna(\"\")\n    return s.tolist()\n\n# Handle OneHotEncoder API difference across sklearn versions\ntry:\n    _ = OneHotEncoder(sparse_output=True)\n    _OHE_KW = {\"sparse_output\": True}\nexcept TypeError:\n    _OHE_KW = {\"sparse\": True}\n\n\n\n2 Preprocessing\n\ntext_pipe = Pipeline([\n    (\"clean\", FunctionTransformer(_clean_text_input, validate=False)),\n    (\"tfidf\", TfidfVectorizer(\n        lowercase=True,\n        max_features=40_000,\n        ngram_range=(1, 2),\n        min_df=5\n    )),\n])\n\ncat_pipe = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", **_OHE_KW)),\n])\n\nnum_pipe = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler(with_mean=False)),\n])\n\npre = ColumnTransformer(\n    transformers=[\n        (\"txt\", text_pipe, [text_col]),  # pass 2-D slice\n        (\"cat\", cat_pipe, cat_cols),\n        (\"num\", num_pipe, num_cols),\n    ],\n    sparse_threshold=1.0\n)\n\nX = pre.fit_transform(df)\nprint(\"Feature matrix shape:\", X.shape)\n\nFeature matrix shape: (72498, 7752)\n\n\n\n\n3 Model Selection\n\nk_values = list(range(4, 11))\ninertias, sils = [], []\n\n# Subsample for silhouette if very large\nif X.shape[0] &gt; 8000:\n    rng = np.random.default_rng(RANDOM_STATE)\n    idx = rng.choice(X.shape[0], size=8000, replace=False)\n    X_sil = X[idx]\nelse:\n    X_sil = X\n\nfor k in k_values:\n    km = KMeans(n_clusters=k, n_init=20, random_state=RANDOM_STATE)\n    km.fit(X)\n    inertias.append(km.inertia_)\n    sils.append(silhouette_score(X_sil, km.predict(X_sil), metric=\"euclidean\"))\n\nplt.figure(figsize=(10,4))\nplt.plot(k_values, inertias, marker=\"o\")\nplt.xlabel(\"k\"); plt.ylabel(\"Inertia\"); plt.title(\"KMeans Elbow\")\nplt.tight_layout(); plt.savefig(\"output/kmeans_elbow.png\", dpi=150); plt.show()\n\nplt.figure(figsize=(10,4))\nplt.plot(k_values, sils, marker=\"o\")\nplt.xlabel(\"k\"); plt.ylabel(\"Silhouette\"); plt.title(\"KMeans Silhouette (subsample)\")\nplt.tight_layout(); plt.savefig(\"output/kmeans_silhouette.png\", dpi=150); plt.show()\n\nbest_k = int(k_values[int(np.argmax(sils))])\nprint(\"Chosen k:\", best_k)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChosen k: 4\n\n\n\n\n4 Final fit\n\nfrom sklearn.decomposition import TruncatedSVD  # used later\n\nkmeans = KMeans(n_clusters=best_k, n_init=20, random_state=RANDOM_STATE)\nlabels = kmeans.fit_predict(X)\n\n# Build a compact frame with inputs + cluster id\ndf_clusters = df[[text_col] + cat_cols + num_cols].copy()\ndf_clusters[\"cluster\"] = labels\n\n# Save outputs for reuse\ndf_clusters.to_csv(\"output/cluster_assignments.csv\", index=False)\n\nsizes = df_clusters[\"cluster\"].value_counts().sort_index()\nprint(\"Cluster sizes:\\n\", sizes)\nsizes.to_csv(\"output/cluster_sizes.csv\")\n\n# --- Top TF-IDF terms per cluster (text portion) ---\ntfidf = pre.named_transformers_[\"txt\"].named_steps[\"tfidf\"]\nterms = np.array(tfidf.get_feature_names_out())\ntext_only = tfidf.transform(_clean_text_input(df[text_col]))\n\ntop_n = 15\ntop_terms = {}\nfor c in range(best_k):\n    mask = (labels == c)\n    if mask.sum() == 0:\n        top_terms[c] = []\n        continue\n    centroid = text_only[mask].mean(axis=0)\n    centroid = np.asarray(centroid).ravel()\n    idx = np.argsort(centroid)[::-1][:top_n]\n    top_terms[c] = terms[idx].tolist()\n\nwith open(\"output/cluster_top_terms.txt\", \"w\") as f:\n    for c in range(best_k):\n        f.write(f\"Cluster {c} â top terms:\\n\")\n        f.write(\", \".join(top_terms[c]) + \"\\n\\n\")\n\nprint(\"Top terms file saved -&gt; output/cluster_top_terms.txt\")\n\nCluster sizes:\n cluster\n0     1673\n1    44869\n2    12657\n3    13299\nName: count, dtype: int64\nTop terms file saved -&gt; output/cluster_top_terms.txt\n\n\n\n\n5 Cluster summary charts\n\nimport matplotlib.patches as mpatches\nfrom matplotlib.ticker import FuncFormatter\n\n# Build dfc in-memory from assignments\ndfc = df_clusters.copy()\n\n# Detect AI terms on TITLE\nTITLE_COL = \"TITLE_CLEAN\" if \"TITLE_CLEAN\" in dfc.columns else \"TITLE\"\nassert TITLE_COL in dfc.columns, \"Need a TITLE or TITLE_CLEAN column in dfc.\"\n\nAI_TERMS = [\n    r\"\\bAI\\b\", r\"\\bML\\b\", r\"\\bLLM\\b\", r\"\\bNLP\\b\",\n    \"artificial intelligence\", \"machine learning\", \"deep learning\",\n    \"computer vision\", \"generative\", \"gen ai\", \"chatgpt\", r\"gpt-\\d+\",\n    \"transformer\", \"bert\", \"prompt engineer\", \"reinforcement learning\"\n]\nai_pat = re.compile(\"|\".join(AI_TERMS), flags=re.IGNORECASE)\nif \"is_ai\" not in dfc.columns:\n    dfc[\"is_ai\"] = dfc[TITLE_COL].astype(str).str.contains(ai_pat, na=False)\n\n# Colors per cluster id\ncluster_colors = {c: plt.cm.get_cmap(\"tab10\")(i % 10) for i, c in enumerate(sorted(dfc[\"cluster\"].unique()))}\n\n# Human-friendly names (fallback to id if not mapped)\ncluster_names = {i: f\"Cluster {i}\" for i in sorted(dfc[\"cluster\"].unique())}\ndfc[\"cluster_name\"] = dfc[\"cluster\"].map(cluster_names)\n\n# Aggregate stats\nstats = (\n    dfc.groupby(\"cluster\", as_index=False)\n       .agg(postings=(\"cluster\",\"size\"),\n            ai_share=(\"is_ai\",\"mean\"),\n            median_salary=(\"SALARY\",\"median\"))\n)\nstats[\"cluster_name\"] = stats[\"cluster\"].map(cluster_names)\nstats[\"label\"] = stats.apply(lambda r: f\"C{int(r.cluster)} â {r.cluster_name}\", axis=1)\nstats[\"ai_share_pct\"] = (stats[\"ai_share\"] * 100).round(1)\nstats[\"color\"] = stats[\"cluster\"].map(cluster_colors)\n\n# Legend/key (reusable)\nlegend_handles = [\n    mpatches.Patch(color=cluster_colors[c], label=f\"C{c}\")\n    for c in sorted(stats[\"cluster\"].unique())\n]\n\n# Plot 1: AI share by cluster\ndfp = stats.sort_values(\"ai_share_pct\", ascending=True)\nplt.figure(figsize=(11, 4))\nplt.barh(dfp[\"label\"], dfp[\"ai_share_pct\"], color=dfp[\"color\"])\nplt.title(\"AI Share by Cluster (% of postings with AI terms)\")\nplt.xlabel(\"AI Share (%)\")\nfor y, v in enumerate(dfp[\"ai_share_pct\"]):\n    plt.text(v + 0.5, y, f\"{v:.1f}%\", va=\"center\")\nplt.legend(handles=legend_handles, title=\"Cluster Key\", loc=\"lower right\")\nplt.tight_layout()\nplt.savefig(\"output/cluster_ai_share.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\n\n# Plot 2: Median salary by cluster\ndfp = stats.sort_values(\"median_salary\", ascending=True)\nplt.figure(figsize=(11, 4))\nplt.barh(dfp[\"label\"], dfp[\"median_salary\"], color=dfp[\"color\"])\nplt.title(\"Median Salary by Cluster\")\nplt.xlabel(\"Salary (USD)\")\nplt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f\"${int(x):,}\"))\nfor y, v in enumerate(dfp[\"median_salary\"]):\n    if pd.notnull(v):\n        plt.text(v, y, f\"${int(v):,}\", va=\"center\", ha=\"left\", fontsize=9)\nplt.legend(handles=legend_handles, title=\"Cluster Key\", loc=\"lower right\")\nplt.tight_layout()\nplt.savefig(\"output/cluster_median_salary.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\n\n# Save tabular summary\nstats_out = stats[[\"cluster\",\"cluster_name\",\"postings\",\"ai_share_pct\",\"median_salary\"]].sort_values(\"cluster\")\nstats_out.to_csv(\"output/cluster_summary.csv\", index=False)\nstats_out.head(10)\n\n/var/folders/7j/ct705g296ls7nrjh30h9pyg40000gn/T/ipykernel_59075/2483979144.py:22: MatplotlibDeprecationWarning:\n\nThe get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncluster\ncluster_name\npostings\nai_share_pct\nmedian_salary\n\n\n\n\n0\n0\nCluster 0\n1673\n0.9\n136400.0\n\n\n1\n1\nCluster 1\n44869\n1.0\n97250.0\n\n\n2\n2\nCluster 2\n12657\n0.7\n103500.0\n\n\n3\n3\nCluster 3\n13299\n1.1\n162500.0\n\n\n\n\n\n\n\n\n\n6 SVD 2D\n\nfrom sklearn.decomposition import TruncatedSVD\n\nassert \"X\" in globals(), \"Feature matrix X missing.\"\nsvd = TruncatedSVD(n_components=2, random_state=RANDOM_STATE)\nXY = svd.fit_transform(X)\nprint(\"Explained variance (2 comps):\", svd.explained_variance_ratio_.sum())\n\nExplained variance (2 comps): 0.3385352006257032\n\n\n\n\n7 Single Cluster Scatter\n\nplt.figure(figsize=(9,6))\nfor c in sorted(np.unique(labels)):\n    m = (labels == c)\n    plt.scatter(XY[m,0], XY[m,1], s=8, alpha=0.5,\n                color=cluster_colors[c], label=f\"C{c}\")\nplt.title(\"KMeans clusters (2-D SVD embedding)\")\nplt.xlabel(\"SVD 1\"); plt.ylabel(\"SVD 2\")\nplt.legend(markerscale=2, frameon=True)\nplt.tight_layout()\nplt.savefig(\"output/kmeans_svd_scatter.png\", dpi=180, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nUnder the recommendation of the Kmeans Elbow and Silhouette measures, four clear segments have emerged. C0 (EA / SAPâOracle Consulting, Sr) comprises enterprise solution owners and senior consultants focused on ERP/CRM integrations, domain architecture, and delivery roadmaps. C1 (Data / BI Analysts) is the highâvolume analytics backbone handling reporting, dashboards, and KPI/adâhoc analysis at midâcareer compensation. C2 (Enterprise / Cloud Architects) is the premium niche cluster with principals and leads who own cloud platforms, reliability/security, and crossâteam technical direction, and therefore command the highest pay. C3 (Data / BI Analysts, consulting tilt) mirrors C1âs skills but skews toward consulting and remote work and shows the highest AIâkeyword incidence, reflecting appliedâAI enablement inside analytics teams. Overall, analyst demand drives scale (C1/C3), enterprise solutioning provides the integration bench (C0), and crossâplatform leadership remains scarce and premium (C2).\n\n\n8 Reference table\n\nfrom sklearn.metrics import (\n    adjusted_rand_score, normalized_mutual_info_score,\n    homogeneity_score, completeness_score, v_measure_score\n)\n\n# Choose a reasonable reference column \nREF_COL = next((c for c in [\n    \"SOC_2021_3_NAME\",\"SOC_2021_2_NAME\",\"SOC_2021_5_NAME\",\n    \"NAICS_2022_4_NAME\",\"NAICS_2022_2_NAME\",\"NAICS_2022_6_NAME\",\n    \"ONET_NAME\",\"ONET_2019_NAME\"\n] if c in df.columns), None)\nassert REF_COL, \"No SOC/NAICS/ONET label column found.\"\n\ndf_clusters = df_clusters.copy()\ndf_clusters[\"ref_label\"] = df[REF_COL].astype(\"string\").fillna(\"Unknown\").values\n\ny_true = df_clusters[\"ref_label\"].astype(str).values\ny_pred = df_clusters[\"cluster\"].astype(int).values\n\nnmi  = normalized_mutual_info_score(y_true, y_pred)\nari  = adjusted_rand_score(y_true, y_pred)\nhom  = homogeneity_score(y_true, y_pred)\ncomp = completeness_score(y_true, y_pred)\nvms  = v_measure_score(y_true, y_pred)\n\nprint(f\"NMI: {nmi:.3f} | ARI: {ari:.3f} | Homogeneity: {hom:.3f} | Completeness: {comp:.3f} | V-measure: {vms:.3f}\")\n\n# Majority label per cluster + purity\nct = pd.crosstab(df_clusters[\"cluster\"], df_clusters[\"ref_label\"])\ncluster_major = ct.idxmax(axis=1).rename(\"majority_label\")\ncluster_hits  = ct.max(axis=1)\npurity = cluster_hits.sum() / ct.values.sum()\n\nsummary = (\n    pd.concat([cluster_major, cluster_hits.rename(\"majority_count\"),\n               ct.sum(axis=1).rename(\"cluster_size\")], axis=1)\n      .assign(majority_share=lambda d: d[\"majority_count\"] / d[\"cluster_size\"])\n      .sort_index()\n)\n\nprint(\"\\nCluster â majority reference label:\")\nsummary.head(best_k)\nprint(f\"\\nOverall purity: {purity:.3f}\")\n\nsummary.to_csv(\"output/cluster_majority_label_summary.csv\")\nct.to_csv(\"output/cluster_label_crosstab.csv\")\n\nNMI: 0.000 | ARI: -0.001 | Homogeneity: 0.038 | Completeness: 0.000 | V-measure: 0.000\n\nCluster â majority reference label:\n\nOverall purity: 0.999\n\n\n\n\n9 Heatmap prep\n\nimport pandas as pd\n\n# Quick diagnostics for label visibility\ncands = [\n    \"NAICS_2022_6_NAME\",\"NAICS_2022_4_NAME\",\"NAICS_2022_2_NAME\",\n    \"ONET_NAME\",\"ONET_2019_NAME\",\n    \"SOC_2021_5_NAME\",\"SOC_2021_3_NAME\",\"SOC_2021_2_NAME\",\n]\ndiag = []\nfor c in cands:\n    if c in df.columns:\n        s = df[c].astype(\"string\")\n        diag.append({\n            \"column\": c,\n            \"non_null_share\": s.notna().mean(),\n            \"n_unique_nonnull\": s.dropna().nunique(),\n            \"top5\": s.value_counts(dropna=True).head(5).index.tolist()\n        })\ndiag_df = pd.DataFrame(diag).sort_values([\"n_unique_nonnull\",\"non_null_share\"], ascending=[False, False])\nprint(\"Label candidates (more uniques is better):\")\ndisplay(diag_df)\n\n# --- Auto-pick with stronger uniqueness requirement ---\n# Prefer columns with decent coverage and at least 5â40 distinct values\nviable = diag_df[(diag_df[\"non_null_share\"] &gt;= 0.40) & (diag_df[\"n_unique_nonnull\"].between(5, 40))]\nif len(viable):\n    REF_COL = viable.iloc[0][\"column\"]\nelse:\n    # Fallback\n    REF_COL = diag_df.iloc[0][\"column\"]\n\n\nprint(\"Using reference label column:\", REF_COL)\n\nLabel candidates (more uniques is better):\n\n\n\n\n\n\n\n\n\ncolumn\nnon_null_share\nn_unique_nonnull\ntop5\n\n\n\n\n0\nNAICS_2022_6_NAME\n0.999393\n814\n[Unclassified Industry, Custom Computer Progra...\n\n\n1\nNAICS_2022_4_NAME\n0.999393\n294\n[Computer Systems Design and Related Services,...\n\n\n2\nNAICS_2022_2_NAME\n0.999393\n21\n[Professional, Scientific, and Technical Servi...\n\n\n3\nONET_NAME\n0.999393\n1\n[Business Intelligence Analysts]\n\n\n4\nONET_2019_NAME\n0.999393\n1\n[Business Intelligence Analysts]\n\n\n5\nSOC_2021_5_NAME\n0.999393\n1\n[Data Scientists]\n\n\n6\nSOC_2021_3_NAME\n0.999393\n1\n[Mathematical Science Occupations]\n\n\n7\nSOC_2021_2_NAME\n0.999393\n1\n[Computer and Mathematical Occupations]\n\n\n\n\n\n\n\nUsing reference label column: NAICS_2022_2_NAME\n\n\n\n\n10 Heatmap plot\n\nimport numpy as np, pandas as pd, os\nimport matplotlib.pyplot as plt\ntry:\n    import seaborn as sns\n    use_sns = True\nexcept Exception:\n    use_sns = False\n\nos.makedirs(\"output\", exist_ok=True)\n\n# Base clustered frame\nif \"dfc\" in globals():\n    base = dfc.copy()\nelif \"df_clusters\" in globals():\n    base = df_clusters.copy()\nelse:\n    raise AssertionError(\"Run the KMeans chunk so dfc/df_clusters exist.\")\n\n# Attach chosen reference label (aligned by row order)\nbase[\"ref_label\"] = df[REF_COL].astype(\"string\").fillna(\"Unknown\").values\n\n# Crosstab clusters x labels\nct = pd.crosstab(base[\"cluster\"], base[\"ref_label\"])\n\n# Drop 'Unknown' column if present\nif \"Unknown\" in ct.columns:\n    ct = ct.drop(columns=[\"Unknown\"])\n\n# Keep top N labels overall (bucket the rest)\nTOPN = 15\nif ct.shape[1] &gt; TOPN:\n    keep = ct.sum(axis=0).sort_values(ascending=False).head(TOPN).index\n    ct_reduced = ct[keep].copy()\n    ct_reduced[\"Other\"] = ct.drop(columns=keep).sum(axis=1)\nelse:\n    ct_reduced = ct\n\n# Remove empty rows (just in case)\nct_reduced = ct_reduced.loc[ct_reduced.sum(axis=1) &gt; 0]\n\n# Convert to row percentages\npct = ct_reduced.div(ct_reduced.sum(axis=1), axis=0) * 100\n\n\nif pct.shape[1] &lt;= 1:\n    print(f\" {REF_COL} doesnât have enough diversity after cleaning. \"\n          f\"Try overriding REF_COL to something like NAICS_2022_4_NAME or ONET_NAME.\")\nelse:\n    plt.figure(figsize=(max(10, 0.7*pct.shape[1]), max(4, 0.6*pct.shape[0])))\n    if use_sns:\n        ax = sns.heatmap(pct, cmap=\"Blues\", annot=True, fmt=\".0f\",\n                         cbar_kws={\"label\": \"% within cluster\"})\n    else:\n        im = plt.imshow(pct.values, aspect=\"auto\", cmap=\"Blues\")\n        plt.colorbar(im, label=\"% within cluster\")\n        plt.xticks(range(pct.shape[1]), pct.columns, rotation=45, ha=\"right\")\n        plt.yticks(range(pct.shape[0]), pct.index)\n        ax = plt.gca()\n\n    plt.title(f\"Cluster vs {REF_COL} (row %)\")\n    plt.xlabel(REF_COL); plt.ylabel(\"Cluster\")\n    plt.tight_layout()\n    plt.savefig(\"output/cluster_vs_reference_heatmap.png\", dpi=200, bbox_inches=\"tight\")\n    plt.show()\n\n/var/folders/7j/ct705g296ls7nrjh30h9pyg40000gn/T/ipykernel_59075/2086638697.py:62: UserWarning:\n\nTight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n\n\n\n\n\n\n\n\n\n\nThis figure is a heatmap of jobâposting shares by industry, displayed as row percentages. The leftmost columnâ that includes Professional, Scientific & Technical Services, shows the darkest shading overall, meaning a large portion of postings in each row come from that industry, while lighter columns represent industries that account for a smaller share. The Profesional, Scientific, and Technical Services industry does seem to be the hottest job market based off of this dataset, however, this does not mean that it is necessarily driven by AI alone.\n\n\n11 Kmeans Summary\nThe results point to a job market organized around four role families with one noticeably premium niche, and they suggest that compensation and hiring dynamics are driven more by seniority and enterprise scope than by AI keywords. Cluster C1 (Analyst & BI Core) supplies most of the volume (â56%), centered on titles like data analyst, business analyst, and BI/analytics; it anchors dayâtoâday decision support and sits near the midâ$110k median. Cluster C3 (Data Analyst & BI â Mixed/Remote, AIâskewed) is a smaller analyst cohort with a stronger remote profile and the highest AIâkeyword incidence; it reflects appliedâAI enablement inside analytics teams and also prices around the midâ$110k band. Cluster C0 (Enterprise Architecture & Solutions) blends architect/enterprise/SAPâOracle/consultant language and tilts toward crossâsystem solution ownershipâERP/CRM modernization, integration roadmaps, and domain architectureâwith compensation likewise clustering near the midâ$110k range. Finally, Cluster C2 (Senior Enterprise/Cloud Leadership) is the small but premium niche (~7%) that stands apart both visually and economically, with a roughly $175k median in the provided charts; employers reward systems ownership, architectural accountability, reliability/security considerations, and crossâteam leadership, even when titles donât carry explicit AI keywords. Meanwhile, AIâtagged titles are increasing from a small base and are unevenly distributedâmost common in C3 and least common in C2, yet the salary premium does not follow that same AI gradient, reinforcing that pay aligns with role family and enterprise scope rather than AI labeling. Industry composition across all clusters is anchored in Professional, Scientific & Technical Services, with steady contributions from Administrative & Support, Finance & Insurance, and Manufacturing. The fastest AI growth is occurring outside of pure softwareâholding companies, motorâvehicle parts, R&D services, executive search, and temporary help which signals diffusion of AI demand into corporate centers and industrial supply chains. This suggests that an increased use of applied AI in operational workforces. Altogether, the evidence suggests a hybrid hiring mix: steady growth of appliedâAI roles embedded in analytics workflows (C1/C3), alongside continued scarcity and premium pricing for crossâfunctional platform leaders (C2) who can translate strategy into architecture and delivery, with C0 providing the enterprise solutions bench that stitches systems and governance together. Although AI is a key component within this analysis, it does not necessarily mean that AI is overtaking the job industry and âdestroying the job marketâ, but rather remodeling the job market and how we use technology within the workplace. AI is steadily contributing to industries such as Administrative & Support, Finance & Insurance, and Manufacturing, however, premium job postings with the highest salaries seem to still value human judgement and ability."
  }
]