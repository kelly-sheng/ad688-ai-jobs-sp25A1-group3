---
title: "Data Analysis"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends"
author:
  - name: Kelly, Sabrina, Makenzie
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
bibliography: references.bib
csl: csl/econometrica.csl
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
---

# Loading Libraries and Data
```{python}
from pyspark.sql import SparkSession
import pandas as pd
import matplotlib.pyplot as plt

raw_df = pd.read_csv("data/lightcast_job_postings.csv")
raw_df.columns.tolist()
```

# Cleaning Data
```{python}
columns_to_drop = [
    "ID", "URL", "ACTIVE_URLS", "DUPLICATES", "LAST_UPDATED_TIMESTAMP",
    "NAICS2", "NAICS3", "NAICS4", "NAICS5", "NAICS6",
    "SOC_2", "SOC_3", "SOC_5"
]
raw_df.drop(columns=columns_to_drop, inplace=True)

# Fill missing values
raw_df["SALARY"].fillna(raw_df["SALARY"].median(), inplace=True)
raw_df["NAICS_2022_6"].fillna("Unknown", inplace=True)

# Drop columns with >50% missing values
raw_df.dropna(thresh=len(raw_df) * 0.5, axis=1, inplace=True)

raw_df = raw_df.drop_duplicates(subset=["TITLE", "COMPANY", "LOCATION", "POSTED"], keep="first")

raw_df.head()
```

# Plot setup for AI vs Non-AI job posting count
```{python}
#| echo: true
#| eval: true
#| error: true
#| df-print: default

import os, re
import pandas as pd

# Kelly’s cleaned dataframe must exist
assert "raw_df" in globals(), "raw_df must exist (Kelly’s cleaned dataframe)."
df = raw_df.copy()

# Parse date -> month
if "POSTED" not in df.columns:
    raise ValueError("Expected a POSTED column in raw_df.")
df["POSTED_DT"] = pd.to_datetime(df["POSTED"], errors="coerce")
df = df.dropna(subset=["POSTED_DT"])
df["month"] = df["POSTED_DT"].dt.to_period("M").dt.to_timestamp()

# Combine likely text fields
candidate_text_cols = [
    "TITLE","TITLE_CLEAN","TITLE_NAME","BODY",
    "SKILLS","SKILLS_NAME",
    "SPECIALIZED_SKILLS","SPECIALIZED_SKILLS_NAME",
    "SOFTWARE_SKILLS","SOFTWARE_SKILLS_NAME",
    "COMMON_SKILLS","COMMON_SKILLS_NAME",
    "CERTIFICATIONS_NAME",
]
text_cols = [c for c in candidate_text_cols if c in df.columns]
if not text_cols:
    text_cols = ["TITLE"] if "TITLE" in df.columns else []
text_series = (
    df[text_cols].astype(str).agg(" ".join, axis=1).str.lower()
    if text_cols else pd.Series([""] * len(df), index=df.index)
)

# ✅ FIXED list (no cut-off strings)
ai_terms = [
    "artificial intelligence","ai","machine learning","deep learning",
    "neural network","nlp","natural language","computer vision",
    "reinforcement learning","generative ai","llm","gpt","chatgpt",
    "transformer","bert","prompt engineer","prompt engineering"
]

# Word-boundary pattern so we don't match 'retail' for 'ai'
ai_pattern = re.compile(
    r"\b(?:"
    + "|".join(re.escape(t) for t in ai_terms)
    + r")\b",
    flags=re.IGNORECASE,
)

df["IS_AI"] = text_series.str.contains(ai_pattern, na=False)

# Monthly counts
monthly = (
    df.groupby(["month","IS_AI"])
      .size()
      .unstack(fill_value=0)
      .rename(columns={True: "AI", False: "Non-AI"})
      .sort_index()
)

monthly_plt = monthly.copy()
monthly_plt.head(12)


```

# Job Postings for AI vs Non-AI Jobs
```{python}
#| echo: true
#| eval: true
#| error: true

import os
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter

assert "monthly_plt" in globals(), "Run the setup chunk first."

os.makedirs("output", exist_ok=True)

plt.figure(figsize=(12, 6))
plt.plot(monthly_plt.index, monthly_plt["AI"],     marker="o", linewidth=2, label="AI jobs")
plt.plot(monthly_plt.index, monthly_plt["Non-AI"], marker="o", linewidth=2, label="Non-AI jobs")
plt.title("AI vs Non-AI Job Postings Over Time (Monthly)")
plt.xlabel("Month")
plt.ylabel("Number of Job Postings")
plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f"{int(x):,}"))
plt.grid(True, alpha=0.25)
plt.legend()
plt.tight_layout()
plt.savefig("output/ai_vs_nonai_over_time.png", dpi=200, bbox_inches="tight")
plt.show()

```

# Prep for monthly AI counts 
```{python}
#| echo: true
#| eval: true
#| error: true
#| df-print: default

import os, re
import numpy as np
import pandas as pd

assert "raw_df" in globals(), "raw_df must exist."

df = raw_df.copy()

# ---- Dates -> month ----
df["POSTED_DT"] = pd.to_datetime(df["POSTED"], errors="coerce")
df = df.dropna(subset=["POSTED_DT"])
df["month"] = df["POSTED_DT"].dt.to_period("M").dt.to_timestamp()

# ---- AI detector (reuse if already present) ----
if "IS_AI" not in df.columns:
    ai_terms = [
        "artificial intelligence","ai","machine learning","deep learning",
        "neural network","nlp","natural language","computer vision",
        "reinforcement learning","generative ai","llm","gpt","chatgpt",
        "transformer","bert","prompt engineer","prompt engineering"
    ]
    text_cols = [c for c in [
        "TITLE","TITLE_CLEAN","BODY",
        "SKILLS","SKILLS_NAME",
        "SPECIALIZED_SKILLS","SPECIALIZED_SKILLS_NAME",
        "SOFTWARE_SKILLS","SOFTWARE_SKILLS_NAME",
        "COMMON_SKILLS","COMMON_SKILLS_NAME",
        "CERTIFICATIONS_NAME"
    ] if c in df.columns]
    combined = (df[text_cols].astype(str).agg(" ".join, axis=1).str.lower()
                if text_cols else pd.Series([""], index=df.index))
    pattern = re.compile(r"\b(?:%s)\b" % "|".join(re.escape(t) for t in ai_terms), re.I)
    df["IS_AI"] = combined.str.contains(pattern, na=False)

# ---- Pick the best available industry label ----
ind_candidates = [
    "NAICS_2022_6_NAME","NAICS6_NAME",
    "NAICS_2022_4_NAME","NAICS4_NAME",
    "NAICS_2022_2_NAME","NAICS2_NAME",
    "NAICS_2022_6","NAICS6"
]
IND_COL = next((c for c in ind_candidates if c in df.columns), None)
if IND_COL is None:
    raise ValueError("No NAICS/industry name/code columns found.")

# ---- Monthly AI counts per industry ----
ai = df[df["IS_AI"]].copy()
ai_monthly = (
    ai.groupby([IND_COL, "month"])
      .size()
      .reset_index(name="count")
)

# ---- Compute growth: (last 3-mo avg - first 3-mo avg) / first 3-mo avg ----
def growth_row(g):
    g = g.sort_values("month")
    k = min(3, len(g))
    first = g["count"].iloc[:k].mean()
    last  = g["count"].iloc[-k:].mean()
    total = g["count"].sum()
    if k < 2 or first == 0:
        return pd.Series({"growth_pct": np.nan, "first_avg": first, "last_avg": last, "months": len(g), "total": total})
    return pd.Series({"growth_pct": (last - first) / first * 100.0,
                      "first_avg": first, "last_avg": last,
                      "months": len(g), "total": total})

growth_df = ai_monthly.groupby(IND_COL).apply(growth_row).reset_index()

# filter out tiny-volume industries to avoid wild % changes
MIN_TOTAL = 30
growth_df = growth_df[growth_df["total"] >= MIN_TOTAL].dropna(subset=["growth_pct"])

# pick top movers (you can adjust top_n)
top_n = 12
growth_top = growth_df.sort_values("growth_pct", ascending=False).head(top_n)

# Save a copy if you want a table in the doc
growth_top_rounded = growth_top.copy()
growth_top_rounded["growth_pct"] = growth_top_rounded["growth_pct"].round(1)
growth_top_rounded.head(top_n)

```

# AI-Driven Job Growth by Industry
```{python}
# --- Short, readable labels + plot & save (Option B) -------------------------
import os, re
from textwrap import shorten
import matplotlib.pyplot as plt

# Use your existing industry column if defined; otherwise default:
IND_COL = IND_COL if "IND_COL" in globals() else "NAICS_2022_6_NAME"

def short_label(s: str) -> str:
    s = str(s)
    # Common abbreviations to keep labels compact but clear
    s = re.sub(r'(?i)\bservices?\b', 'Svcs', s)
    s = re.sub(r'(?i)\bmanufacturing\b', 'Mfg', s)
    s = re.sub(r'(?i)\bpreparation\b', 'Prep', s)
    s = re.sub(r'(?i)\bmanagement\b', 'Mgmt', s)
    s = re.sub(r'(?i)\bcomputer\b', 'Comp', s)
    s = re.sub(r'(?i)\bequipment\b', 'Equip', s)
    s = re.sub(r'(?i)\bwholesale\b', 'Whsl', s)
    s = re.sub(r'(?i)\bagents?\b', 'Agts', s)
    s = re.sub(r'(?i)\bscientific\b', 'Sci', s)
    s = re.sub(r'(?i)\bengineering\b', 'Eng', s)
    s = re.sub(r'(?i)\band\b', '&', s)
    s = re.sub(r'(?i)\bother\b\s*', '', s)  # drop leading "Other"
    # Final safe trim
    return shorten(s.strip(), width=32, placeholder='…')

plot_df = growth_top.copy()
plot_df["label"] = plot_df[IND_COL].astype(str).map(short_label)

fig, ax = plt.subplots(figsize=(11, 7))
bars = ax.barh(plot_df["label"], plot_df["growth_pct"], color="#2F6DB3", edgecolor="black")
ax.invert_yaxis()  # biggest at top

ax.set_title("AI Job Posting Growth by Industry (First vs Last Month)", fontsize=16)
ax.set_xlabel("Growth (%)", fontsize=12)
ax.set_ylabel("Industry", fontsize=12)
ax.grid(axis="x", alpha=0.25)

# Value labels
for b in bars:
    w = b.get_width()
    ax.text(
        w + (1 if w >= 0 else -1),
        b.get_y() + b.get_height() / 2,
        f"{w:.1f}%",
        va="center",
        ha="left" if w >= 0 else "right",
        fontsize=10,
    )

plt.tight_layout()
os.makedirs("output", exist_ok=True)
plt.savefig("output/ai_industry_growth_shortlabels.png", dpi=200, bbox_inches="tight")
plt.show()

```

# Non-AI Job Posting Growth by Industry setup
```{python}
#| echo: true
#| eval: true
#| error: true

import os, re
import pandas as pd
from textwrap import shorten

assert "raw_df" in globals(), "raw_df must exist (your cleaned dataframe)."
df = raw_df.copy()

# Pick an industry-name column that exists
IND_COL = next((c for c in [
    "NAICS_2022_6_NAME","NAICS6_NAME","NAICS_2022_4_NAME","NAICS4_NAME",
    "NAICS_2022_2_NAME","NAICS2_NAME"
] if c in df.columns), None)
assert IND_COL, "No industry/NAICS name column found."

TITLE_COL = "TITLE_CLEAN" if "TITLE_CLEAN" in df.columns else "TITLE"
assert TITLE_COL in df.columns, "Need a TITLE or TITLE_CLEAN column."

# Dates → month
df["POSTED_DT"] = pd.to_datetime(df["POSTED"], errors="coerce")
df = df.dropna(subset=["POSTED_DT", IND_COL])
df["month"] = df["POSTED_DT"].dt.to_period("M").dt.to_timestamp()

# Detect AI vs non-AI via title keywords
AI_TERMS = [
    "artificial intelligence", r"\bAI\b", "machine learning", r"\bML\b",
    "deep learning", r"\bLLM\b", r"\bNLP\b", "computer vision",
    "generative", "chatgpt", r"gpt-\d+", "transformer", "bert",
    "prompt engineer", "reinforcement learning"
]
ai_pat = re.compile("|".join(AI_TERMS), flags=re.IGNORECASE)
if "is_ai" not in df.columns:
    df["is_ai"] = df[TITLE_COL].astype(str).str.contains(ai_pat, na=False)

# Use industries from your AI-growth visual if available; otherwise pick top 10 by AI postings last month
if "growth_top" in globals():
    target_inds = [str(x) for x in growth_top[IND_COL].dropna().tolist()]
else:
    last_m = df["month"].max()
    ai_last = (df[df["is_ai"] & (df["month"] == last_m)]
               .groupby(IND_COL).size().sort_values(ascending=False).head(10))
    target_inds = ai_last.index.astype(str).tolist()

# Build non-AI growth (% first→last month) for those industries
non_ai = df[(~df["is_ai"]) & (df[IND_COL].astype(str).isin(target_inds))].copy()
monthly = (non_ai.groupby([IND_COL, "month"]).size().reset_index(name="count"))

first_last = (monthly.sort_values("month")
                     .groupby(IND_COL, as_index=False)
                     .agg(first=("count","first"), last=("count","last")))
first_last = first_last[first_last["first"] > 0].copy()
first_last["growth_pct"] = ((first_last["last"] - first_last["first"])
                            / first_last["first"]) * 100.0
first_last = first_last[first_last[IND_COL].astype(str).isin(target_inds)]

def short_label(s: str) -> str:
    s = str(s)
    s = re.sub(r'(?i)\bservices?\b', 'Svcs', s)
    s = re.sub(r'(?i)\bmanufacturing\b', 'Mfg', s)
    s = re.sub(r'(?i)\bmanagement\b', 'Mgmt', s)
    s = re.sub(r'(?i)\bpreparation\b', 'Prep', s)
    s = re.sub(r'(?i)\bcomputer\b', 'Comp', s)
    s = re.sub(r'(?i)\bequipment\b', 'Equip', s)
    s = re.sub(r'(?i)\bwholesale\b', 'Whsl', s)
    s = re.sub(r'(?i)\band\b', '&', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return shorten(s, width=32, placeholder='…')

non_ai_growth_plot_df = (first_last
                         .sort_values("growth_pct", ascending=False)
                         .assign(label=lambda d: d[IND_COL].map(short_label)))

os.makedirs("output", exist_ok=True)
non_ai_growth_plot_df.head(10)  # preview table

```

# Non-AI Job Posting Growth by Industry
```{python}
#| echo: true
#| eval: true
#| error: true

import os
import matplotlib.pyplot as plt

# If prep didn't run in this kernel, rebuild quickly
if "non_ai_growth_plot_df" not in globals():
    # Re-run a minimal rebuild using the same logic
    assert "raw_df" in globals(), "raw_df is required to rebuild plot data."
    # You can just execute the prep cell above; keeping this here for resilience:
    # (We simply import the name if it exists; otherwise advise to run prep.)
    raise AssertionError("Run the prep chunk first to create non_ai_growth_plot_df.")

dfp = non_ai_growth_plot_df.copy()

if dfp.empty:
    print("non_ai_growth_plot_df is empty—nothing to plot. "
          "Check that selected industries have non-AI rows in the first/last month.")
else:
    os.makedirs("output", exist_ok=True)
    colors = dfp["growth_pct"].ge(0).map({True: "#1f77b4", False: "#d62728"}).to_numpy()

    fig, ax = plt.subplots(figsize=(11, 7))
    bars = ax.barh(dfp["label"], dfp["growth_pct"], color=colors, edgecolor="black")

    ax.axvline(0, color="black", linewidth=1, alpha=0.6)
    ax.invert_yaxis()
    ax.set_title("Non-AI Job Posting Growth by Industry (First → Last Month)", fontsize=16)
    ax.set_xlabel("Growth (%)")
    ax.set_ylabel("Industry")
    ax.grid(axis="x", alpha=0.25)

    for b, v in zip(bars, dfp["growth_pct"]):
        ax.text(
            v + (1 if v >= 0 else -1),
            b.get_y() + b.get_height() / 2,
            f"{v:.1f}%",
            va="center",
            ha="left" if v >= 0 else "right",
            fontsize=10,
        )

    plt.tight_layout()
    plt.savefig("output/non_ai_growth_pct_in_ai_growth_industries_colored.png",
                dpi=200, bbox_inches="tight")
    plt.show()

```

# AI-powered roles vs non-ai roles salary prep
```{python}
#| echo: true
#| eval: true
#| error: true

import os, re
import numpy as np
import pandas as pd

assert "raw_df" in globals(), "raw_df (cleaned dataframe) must exist."

df = raw_df.copy()

# --- Pick columns robustly ---
TITLE_COL = "TITLE_CLEAN" if "TITLE_CLEAN" in df.columns else "TITLE"
assert TITLE_COL in df.columns, "Need a TITLE or TITLE_CLEAN column."

IND_COL = next((c for c in [
    "NAICS_2022_6_NAME","NAICS6_NAME","NAICS_2022_4_NAME","NAICS4_NAME",
    "NAICS_2022_2_NAME","NAICS2_NAME","LIGHTCAST_SECTORS_NAME"
] if c in df.columns), None)
assert IND_COL, "No industry/NAICS name column found."

# Salary: prefer SALARY; otherwise average of FROM/TO if present
sal = pd.to_numeric(df.get("SALARY", np.nan), errors="coerce")
if sal.isna().all() and {"SALARY_FROM","SALARY_TO"} <= set(df.columns):
    s_from = pd.to_numeric(df["SALARY_FROM"], errors="coerce")
    s_to   = pd.to_numeric(df["SALARY_TO"],   errors="coerce")
    sal = (s_from + s_to) / 2
df["salary_num"] = pd.to_numeric(sal, errors="coerce")

# keep positive salaries only
df = df[df["salary_num"] > 0].copy()

# Optional: cap extreme outliers so a few posts don't dominate the mean
q_low, q_hi = df["salary_num"].quantile([0.01, 0.99])
df["salary_num"] = df["salary_num"].clip(q_low, q_hi)

# --- Tag AI vs non-AI using title keywords ---
AI_TERMS = [
    "artificial intelligence", r"\bAI\b", "machine learning", r"\bML\b",
    "deep learning", r"\bLLM\b", r"\bNLP\b", "computer vision",
    "generative", "chatgpt", r"gpt-\d+", "transformer", r"\bbert\b",
    "prompt engineer", "reinforcement learning", "data scientist", "ai engineer"
]
ai_pat = re.compile("|".join(AI_TERMS), flags=re.IGNORECASE)
df["is_ai"] = df[TITLE_COL].astype(str).str.contains(ai_pat, na=False)

# --- Find top industries by AI posting count (choose N)
N = 10
ai_counts = (df[df["is_ai"]]
             .groupby(IND_COL, dropna=True)
             .size()
             .sort_values(ascending=False)
             .head(N)
             .rename("ai_posts")
             .reset_index())
top_inds = ai_counts[IND_COL].astype(str).tolist()

# --- Compute average salary (AI vs non-AI) for those industries
subset = df[df[IND_COL].astype(str).isin(top_inds)].copy()
grp = subset.groupby([IND_COL, "is_ai"])["salary_num"].agg(["mean", "count"]).reset_index()

ai_part  = grp[grp["is_ai"]].rename(columns={"mean":"avg_ai_salary", "count":"ai_postings"})
non_part = grp[~grp["is_ai"]].rename(columns={"mean":"avg_nonai_salary", "count":"non_ai_postings"})

ai_vs_trad_industry_salary = (
    ai_counts[[IND_COL, "ai_posts"]]        # preserves AI-based ordering
    .merge(ai_part[[IND_COL, "avg_ai_salary", "ai_postings"]], on=IND_COL, how="left")
    .merge(non_part[[IND_COL, "avg_nonai_salary", "non_ai_postings"]], on=IND_COL, how="left")
)

# Clean up & preview
ai_vs_trad_industry_salary = ai_vs_trad_industry_salary.fillna(0)
os.makedirs("output", exist_ok=True)
ai_vs_trad_industry_salary.head(N)

```

# AI-powered Roles Salaries vs. Non-AI Roles Salaries
```{python}
#| echo: true
#| eval: true
#| error: true

import os, re
import matplotlib.pyplot as plt
from textwrap import shorten
from matplotlib.ticker import FuncFormatter

assert "ai_vs_trad_industry_salary" in globals(), "Run the prep chunk first."

dfp = ai_vs_trad_industry_salary.copy()
if dfp.empty:
    print("No data to plot after filtering. Check AI detection or industry column.")
else:
    # shorten very long industry labels
    def short_label(s: str) -> str:
        s = str(s)
        s = re.sub(r'(?i)\bservices?\b', 'Svcs', s)
        s = re.sub(r'(?i)\bmanufacturing\b', 'Mfg', s)
        s = re.sub(r'(?i)\bmanagement\b', 'Mgmt', s)
        s = re.sub(r'(?i)\bcomputer\b', 'Comp', s)
        s = re.sub(r'(?i)\bequipment\b', 'Equip', s)
        s = re.sub(r'\s+', ' ', s).strip()
        return shorten(s, width=36, placeholder='…')

    dfp["label"] = dfp[IND_COL].map(short_label)

    x = range(len(dfp))
    w = 0.38

    fig, ax = plt.subplots(figsize=(14, 7))
    b1 = ax.bar([i - w/2 for i in x], dfp["avg_ai_salary"],    width=w, label="AI job posts",     edgecolor="black")
    b2 = ax.bar([i + w/2 for i in x], dfp["avg_nonai_salary"], width=w, label="Non-AI job posts", edgecolor="black")

    ax.set_title("Average Salary — AI vs. Non-AI Posts in Top AI Industries", fontsize=16)
    ax.set_xlabel("Industry (Top by AI Posting Volume)")
    ax.set_ylabel("Average Salary ($)")
    ax.set_xticks(list(x))
    ax.set_xticklabels(dfp["label"], rotation=35, ha="right")
    ax.yaxis.set_major_formatter(FuncFormatter(lambda v, pos: f"${int(v):,}"))
    ax.grid(axis="y", alpha=0.25)
    ax.legend()

    # annotate bars
    for bars in (b1, b2):
        for p in bars:
            h = p.get_height()
            if h > 0:
                ax.text(p.get_x()+p.get_width()/2, h, f"${int(h):,}",
                        ha="center", va="bottom", fontsize=9)

    plt.tight_layout()
    os.makedirs("output", exist_ok=True)
    plt.savefig("output/ai_vs_nonai_avg_salary_by_industry.png", dpi=200, bbox_inches="tight")
    plt.show()

```

# Kmeans Clustering Setup
```{python}
#| echo: true
#| error: true

import os, re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

os.makedirs("output", exist_ok=True)

# Use your cleaned frame if present; else load CSV
try:
    df = raw_df.copy()
except NameError:
    df = pd.read_csv("data/lightcast_job_postings.csv")

# Pick a title/text column robustly
for c in ["TITLE_CLEAN", "TITLE", "TITLE_NAME", "TITLE_RAW"]:
    if c in df.columns:
        text_col = c
        break
else:
    raise ValueError("No title column found (TITLE_CLEAN/TITLE/TITLE_NAME/TITLE_RAW).")

# Coerce useful numerics if present
for c in ["SALARY", "SALARY_FROM", "SALARY_TO", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "DURATION"]:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")

# Candidate categoricals (kept if present & not too wide)
candidate_cat = [
    "REMOTE_TYPE_NAME", "STATE_NAME", "EMPLOYMENT_TYPE_NAME",
    "COMPANY_IS_STAFFING", "NAICS_2022_6_NAME", "ONET_NAME", "SOC_2021_5_NAME"
]
cat_cols = [c for c in candidate_cat if c in df.columns]
cat_cols = [c for c in cat_cols if df[c].nunique(dropna=True) <= 200]

num_cols = [c for c in ["SALARY", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "DURATION"] if c in df.columns]

print("Using columns:")
print("  text_col:", text_col)
print("  cat_cols:", cat_cols)
print("  num_cols:", num_cols)

RANDOM_STATE = 42

```
# Helpers
```{python}
#| echo: true
#| error: true

def _clean_text_input(x):
    """
    Accept Series, 1-col DataFrame, or numpy array from ColumnTransformer
    and return a plain Python list[str] with NaNs -> "".
    """
    if isinstance(x, pd.Series):
        s = x
    elif isinstance(x, pd.DataFrame):
        s = x.iloc[:, 0]
    elif isinstance(x, np.ndarray):
        s = pd.Series(x.ravel())
    else:
        s = pd.Series(x)
    s = s.astype("string").fillna("")
    return s.tolist()

# Handle OneHotEncoder API difference across sklearn versions
try:
    _ = OneHotEncoder(sparse_output=True)
    _OHE_KW = {"sparse_output": True}
except TypeError:
    _OHE_KW = {"sparse": True}

```

# Preprocessing
```{python}
#| echo: true
#| error: true

text_pipe = Pipeline([
    ("clean", FunctionTransformer(_clean_text_input, validate=False)),
    ("tfidf", TfidfVectorizer(
        lowercase=True,
        max_features=40_000,
        ngram_range=(1, 2),
        min_df=5
    )),
])

cat_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("ohe", OneHotEncoder(handle_unknown="ignore", **_OHE_KW)),
])

num_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False)),
])

pre = ColumnTransformer(
    transformers=[
        ("txt", text_pipe, [text_col]),  # pass 2-D slice
        ("cat", cat_pipe, cat_cols),
        ("num", num_pipe, num_cols),
    ],
    sparse_threshold=1.0
)

X = pre.fit_transform(df)
print("Feature matrix shape:", X.shape)

```
# Model Selection
```{python}
#| echo: true
#| error: true

k_values = list(range(4, 11))
inertias, sils = [], []

# Subsample for silhouette if very large
if X.shape[0] > 8000:
    rng = np.random.default_rng(RANDOM_STATE)
    idx = rng.choice(X.shape[0], size=8000, replace=False)
    X_sil = X[idx]
else:
    X_sil = X

for k in k_values:
    km = KMeans(n_clusters=k, n_init=20, random_state=RANDOM_STATE)
    km.fit(X)
    inertias.append(km.inertia_)
    sils.append(silhouette_score(X_sil, km.predict(X_sil), metric="euclidean"))

plt.figure(figsize=(10,4))
plt.plot(k_values, inertias, marker="o")
plt.xlabel("k"); plt.ylabel("Inertia"); plt.title("KMeans Elbow")
plt.tight_layout(); plt.savefig("output/kmeans_elbow.png", dpi=150); plt.show()

plt.figure(figsize=(10,4))
plt.plot(k_values, sils, marker="o")
plt.xlabel("k"); plt.ylabel("Silhouette"); plt.title("KMeans Silhouette (subsample)")
plt.tight_layout(); plt.savefig("output/kmeans_silhouette.png", dpi=150); plt.show()

best_k = int(k_values[int(np.argmax(sils))])
print("Chosen k:", best_k)

```

# Final fit
```{python}
#| echo: true
#| error: true

from sklearn.decomposition import TruncatedSVD  # used later

kmeans = KMeans(n_clusters=best_k, n_init=20, random_state=RANDOM_STATE)
labels = kmeans.fit_predict(X)

# Build a compact frame with inputs + cluster id
df_clusters = df[[text_col] + cat_cols + num_cols].copy()
df_clusters["cluster"] = labels

# Save outputs for reuse
df_clusters.to_csv("output/cluster_assignments.csv", index=False)

sizes = df_clusters["cluster"].value_counts().sort_index()
print("Cluster sizes:\n", sizes)
sizes.to_csv("output/cluster_sizes.csv")

# --- Top TF-IDF terms per cluster (text portion) ---
tfidf = pre.named_transformers_["txt"].named_steps["tfidf"]
terms = np.array(tfidf.get_feature_names_out())
text_only = tfidf.transform(_clean_text_input(df[text_col]))

top_n = 15
top_terms = {}
for c in range(best_k):
    mask = (labels == c)
    if mask.sum() == 0:
        top_terms[c] = []
        continue
    centroid = text_only[mask].mean(axis=0)
    centroid = np.asarray(centroid).ravel()
    idx = np.argsort(centroid)[::-1][:top_n]
    top_terms[c] = terms[idx].tolist()

with open("output/cluster_top_terms.txt", "w") as f:
    for c in range(best_k):
        f.write(f"Cluster {c} — top terms:\n")
        f.write(", ".join(top_terms[c]) + "\n\n")

print("Top terms file saved -> output/cluster_top_terms.txt")

```

# Cluster summary charts
```{python}
#| echo: true
#| error: true

import matplotlib.patches as mpatches
from matplotlib.ticker import FuncFormatter

# Build dfc in-memory from assignments
dfc = df_clusters.copy()

# Detect AI terms on TITLE
TITLE_COL = "TITLE_CLEAN" if "TITLE_CLEAN" in dfc.columns else "TITLE"
assert TITLE_COL in dfc.columns, "Need a TITLE or TITLE_CLEAN column in dfc."

AI_TERMS = [
    r"\bAI\b", r"\bML\b", r"\bLLM\b", r"\bNLP\b",
    "artificial intelligence", "machine learning", "deep learning",
    "computer vision", "generative", "gen ai", "chatgpt", r"gpt-\d+",
    "transformer", "bert", "prompt engineer", "reinforcement learning"
]
ai_pat = re.compile("|".join(AI_TERMS), flags=re.IGNORECASE)
if "is_ai" not in dfc.columns:
    dfc["is_ai"] = dfc[TITLE_COL].astype(str).str.contains(ai_pat, na=False)

# Colors per cluster id
cluster_colors = {c: plt.cm.get_cmap("tab10")(i % 10) for i, c in enumerate(sorted(dfc["cluster"].unique()))}

# Human-friendly names (fallback to id if not mapped)
cluster_names = {i: f"Cluster {i}" for i in sorted(dfc["cluster"].unique())}
dfc["cluster_name"] = dfc["cluster"].map(cluster_names)

# Aggregate stats
stats = (
    dfc.groupby("cluster", as_index=False)
       .agg(postings=("cluster","size"),
            ai_share=("is_ai","mean"),
            median_salary=("SALARY","median"))
)
stats["cluster_name"] = stats["cluster"].map(cluster_names)
stats["label"] = stats.apply(lambda r: f"C{int(r.cluster)} — {r.cluster_name}", axis=1)
stats["ai_share_pct"] = (stats["ai_share"] * 100).round(1)
stats["color"] = stats["cluster"].map(cluster_colors)

# Legend/key (reusable)
legend_handles = [
    mpatches.Patch(color=cluster_colors[c], label=f"C{c}")
    for c in sorted(stats["cluster"].unique())
]

# Plot 1: AI share by cluster
dfp = stats.sort_values("ai_share_pct", ascending=True)
plt.figure(figsize=(11, 4))
plt.barh(dfp["label"], dfp["ai_share_pct"], color=dfp["color"])
plt.title("AI Share by Cluster (% of postings with AI terms)")
plt.xlabel("AI Share (%)")
for y, v in enumerate(dfp["ai_share_pct"]):
    plt.text(v + 0.5, y, f"{v:.1f}%", va="center")
plt.legend(handles=legend_handles, title="Cluster Key", loc="lower right")
plt.tight_layout()
plt.savefig("output/cluster_ai_share.png", dpi=200, bbox_inches="tight")
plt.show()

# Plot 2: Median salary by cluster
dfp = stats.sort_values("median_salary", ascending=True)
plt.figure(figsize=(11, 4))
plt.barh(dfp["label"], dfp["median_salary"], color=dfp["color"])
plt.title("Median Salary by Cluster")
plt.xlabel("Salary (USD)")
plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f"${int(x):,}"))
for y, v in enumerate(dfp["median_salary"]):
    if pd.notnull(v):
        plt.text(v, y, f"${int(v):,}", va="center", ha="left", fontsize=9)
plt.legend(handles=legend_handles, title="Cluster Key", loc="lower right")
plt.tight_layout()
plt.savefig("output/cluster_median_salary.png", dpi=200, bbox_inches="tight")
plt.show()

# Save tabular summary
stats_out = stats[["cluster","cluster_name","postings","ai_share_pct","median_salary"]].sort_values("cluster")
stats_out.to_csv("output/cluster_summary.csv", index=False)
stats_out.head(10)

```

# SVD 2D
```{python}
#| label: svd-2d
#| echo: true
#| error: true

from sklearn.decomposition import TruncatedSVD

assert "X" in globals(), "Feature matrix X missing."
svd = TruncatedSVD(n_components=2, random_state=RANDOM_STATE)
XY = svd.fit_transform(X)
print("Explained variance (2 comps):", svd.explained_variance_ratio_.sum())


```

# Single Cluster Scatter
```{python}
#| echo: true
#| error: true
#| dependson: [svd-2d]

plt.figure(figsize=(9,6))
for c in sorted(np.unique(labels)):
    m = (labels == c)
    plt.scatter(XY[m,0], XY[m,1], s=8, alpha=0.5,
                color=cluster_colors[c], label=f"C{c}")
plt.title("KMeans clusters (2-D SVD embedding)")
plt.xlabel("SVD 1"); plt.ylabel("SVD 2")
plt.legend(markerscale=2, frameon=True)
plt.tight_layout()
plt.savefig("output/kmeans_svd_scatter.png", dpi=180, bbox_inches="tight")
plt.show()

```
# Reference table
```{python}
#| echo: true
#| error: true

from sklearn.metrics import (
    adjusted_rand_score, normalized_mutual_info_score,
    homogeneity_score, completeness_score, v_measure_score
)

# Choose a reasonable reference column 
REF_COL = next((c for c in [
    "SOC_2021_3_NAME","SOC_2021_2_NAME","SOC_2021_5_NAME",
    "NAICS_2022_4_NAME","NAICS_2022_2_NAME","NAICS_2022_6_NAME",
    "ONET_NAME","ONET_2019_NAME"
] if c in df.columns), None)
assert REF_COL, "No SOC/NAICS/ONET label column found."

df_clusters = df_clusters.copy()
df_clusters["ref_label"] = df[REF_COL].astype("string").fillna("Unknown").values

y_true = df_clusters["ref_label"].astype(str).values
y_pred = df_clusters["cluster"].astype(int).values

nmi  = normalized_mutual_info_score(y_true, y_pred)
ari  = adjusted_rand_score(y_true, y_pred)
hom  = homogeneity_score(y_true, y_pred)
comp = completeness_score(y_true, y_pred)
vms  = v_measure_score(y_true, y_pred)

print(f"NMI: {nmi:.3f} | ARI: {ari:.3f} | Homogeneity: {hom:.3f} | Completeness: {comp:.3f} | V-measure: {vms:.3f}")

# Majority label per cluster + purity
ct = pd.crosstab(df_clusters["cluster"], df_clusters["ref_label"])
cluster_major = ct.idxmax(axis=1).rename("majority_label")
cluster_hits  = ct.max(axis=1)
purity = cluster_hits.sum() / ct.values.sum()

summary = (
    pd.concat([cluster_major, cluster_hits.rename("majority_count"),
               ct.sum(axis=1).rename("cluster_size")], axis=1)
      .assign(majority_share=lambda d: d["majority_count"] / d["cluster_size"])
      .sort_index()
)

print("\nCluster → majority reference label:")
summary.head(best_k)
print(f"\nOverall purity: {purity:.3f}")

summary.to_csv("output/cluster_majority_label_summary.csv")
ct.to_csv("output/cluster_label_crosstab.csv")

```

# Heatmap prep
```{python}
#| echo: true
#| error: true

import pandas as pd

# Quick diagnostics for label visibility
cands = [
    "NAICS_2022_6_NAME","NAICS_2022_4_NAME","NAICS_2022_2_NAME",
    "ONET_NAME","ONET_2019_NAME",
    "SOC_2021_5_NAME","SOC_2021_3_NAME","SOC_2021_2_NAME",
]
diag = []
for c in cands:
    if c in df.columns:
        s = df[c].astype("string")
        diag.append({
            "column": c,
            "non_null_share": s.notna().mean(),
            "n_unique_nonnull": s.dropna().nunique(),
            "top5": s.value_counts(dropna=True).head(5).index.tolist()
        })
diag_df = pd.DataFrame(diag).sort_values(["n_unique_nonnull","non_null_share"], ascending=[False, False])
print("Label candidates (more uniques is better):")
display(diag_df)

# --- Auto-pick with stronger uniqueness requirement ---
# Prefer columns with decent coverage and at least 5–40 distinct values
viable = diag_df[(diag_df["non_null_share"] >= 0.40) & (diag_df["n_unique_nonnull"].between(5, 40))]
if len(viable):
    REF_COL = viable.iloc[0]["column"]
else:
    # Fallback
    REF_COL = diag_df.iloc[0]["column"]


print("Using reference label column:", REF_COL)

```

# Heatmap plot
```{python}
#| echo: true
#| error: true

import numpy as np, pandas as pd, os
import matplotlib.pyplot as plt
try:
    import seaborn as sns
    use_sns = True
except Exception:
    use_sns = False

os.makedirs("output", exist_ok=True)

# Base clustered frame
if "dfc" in globals():
    base = dfc.copy()
elif "df_clusters" in globals():
    base = df_clusters.copy()
else:
    raise AssertionError("Run the KMeans chunk so dfc/df_clusters exist.")

# Attach chosen reference label (aligned by row order)
base["ref_label"] = df[REF_COL].astype("string").fillna("Unknown").values

# Crosstab clusters x labels
ct = pd.crosstab(base["cluster"], base["ref_label"])

# Drop 'Unknown' column if present
if "Unknown" in ct.columns:
    ct = ct.drop(columns=["Unknown"])

# Keep top N labels overall (bucket the rest)
TOPN = 15
if ct.shape[1] > TOPN:
    keep = ct.sum(axis=0).sort_values(ascending=False).head(TOPN).index
    ct_reduced = ct[keep].copy()
    ct_reduced["Other"] = ct.drop(columns=keep).sum(axis=1)
else:
    ct_reduced = ct

# Remove empty rows (just in case)
ct_reduced = ct_reduced.loc[ct_reduced.sum(axis=1) > 0]

# Convert to row percentages
pct = ct_reduced.div(ct_reduced.sum(axis=1), axis=0) * 100


if pct.shape[1] <= 1:
    print(f"⚠️  {REF_COL} doesn’t have enough diversity after cleaning. "
          f"Try overriding REF_COL to something like NAICS_2022_4_NAME or ONET_NAME.")
else:
    plt.figure(figsize=(max(10, 0.7*pct.shape[1]), max(4, 0.6*pct.shape[0])))
    if use_sns:
        ax = sns.heatmap(pct, cmap="Blues", annot=True, fmt=".0f",
                         cbar_kws={"label": "% within cluster"})
    else:
        im = plt.imshow(pct.values, aspect="auto", cmap="Blues")
        plt.colorbar(im, label="% within cluster")
        plt.xticks(range(pct.shape[1]), pct.columns, rotation=45, ha="right")
        plt.yticks(range(pct.shape[0]), pct.index)
        ax = plt.gca()

    plt.title(f"Cluster vs {REF_COL} (row %)")
    plt.xlabel(REF_COL); plt.ylabel("Cluster")
    plt.tight_layout()
    plt.savefig("output/cluster_vs_reference_heatmap.png", dpi=200, bbox_inches="tight")
    plt.show()
   

```