
# Kmeans Clustering Setup

```{python}
#| echo: true
#| error: true

import os, re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

os.makedirs("output", exist_ok=True)

# Use your cleaned frame if present; else load CSV
try:
    df = raw_df.copy()
except NameError:
    df = pd.read_csv("data/lightcast_job_postings.csv")

# Pick a title/text column robustly
for c in ["TITLE_CLEAN", "TITLE", "TITLE_NAME", "TITLE_RAW"]:
    if c in df.columns:
        text_col = c
        break
else:
    raise ValueError("No title column found (TITLE_CLEAN/TITLE/TITLE_NAME/TITLE_RAW).")

# Coerce useful numerics if present
for c in ["SALARY", "SALARY_FROM", "SALARY_TO", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "DURATION"]:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")

# Candidate categoricals (kept if present & not too wide)
candidate_cat = [
    "REMOTE_TYPE_NAME", "STATE_NAME", "EMPLOYMENT_TYPE_NAME",
    "COMPANY_IS_STAFFING", "NAICS_2022_6_NAME", "ONET_NAME", "SOC_2021_5_NAME"
]
cat_cols = [c for c in candidate_cat if c in df.columns]
cat_cols = [c for c in cat_cols if df[c].nunique(dropna=True) <= 200]

num_cols = [c for c in ["SALARY", "MIN_YEARS_EXPERIENCE", "MAX_YEARS_EXPERIENCE", "DURATION"] if c in df.columns]

print("Using columns:")
print("  text_col:", text_col)
print("  cat_cols:", cat_cols)
print("  num_cols:", num_cols)

RANDOM_STATE = 42

```
# Helpers
```{python}
#| echo: true
#| error: true

def _clean_text_input(x):
    """
    Accept Series, 1-col DataFrame, or numpy array from ColumnTransformer
    and return a plain Python list[str] with NaNs -> "".
    """
    if isinstance(x, pd.Series):
        s = x
    elif isinstance(x, pd.DataFrame):
        s = x.iloc[:, 0]
    elif isinstance(x, np.ndarray):
        s = pd.Series(x.ravel())
    else:
        s = pd.Series(x)
    s = s.astype("string").fillna("")
    return s.tolist()

# Handle OneHotEncoder API difference across sklearn versions
try:
    _ = OneHotEncoder(sparse_output=True)
    _OHE_KW = {"sparse_output": True}
except TypeError:
    _OHE_KW = {"sparse": True}

```

# Preprocessing
```{python}
#| echo: true
#| error: true

text_pipe = Pipeline([
    ("clean", FunctionTransformer(_clean_text_input, validate=False)),
    ("tfidf", TfidfVectorizer(
        lowercase=True,
        max_features=40_000,
        ngram_range=(1, 2),
        min_df=5
    )),
])

cat_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("ohe", OneHotEncoder(handle_unknown="ignore", **_OHE_KW)),
])

num_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False)),
])

pre = ColumnTransformer(
    transformers=[
        ("txt", text_pipe, [text_col]),  # pass 2-D slice
        ("cat", cat_pipe, cat_cols),
        ("num", num_pipe, num_cols),
    ],
    sparse_threshold=1.0
)

X = pre.fit_transform(df)
print("Feature matrix shape:", X.shape)

```
# Model Selection
```{python}
#| echo: true
#| error: true

k_values = list(range(4, 11))
inertias, sils = [], []

# Subsample for silhouette if very large
if X.shape[0] > 8000:
    rng = np.random.default_rng(RANDOM_STATE)
    idx = rng.choice(X.shape[0], size=8000, replace=False)
    X_sil = X[idx]
else:
    X_sil = X

for k in k_values:
    km = KMeans(n_clusters=k, n_init=20, random_state=RANDOM_STATE)
    km.fit(X)
    inertias.append(km.inertia_)
    sils.append(silhouette_score(X_sil, km.predict(X_sil), metric="euclidean"))

plt.figure(figsize=(10,4))
plt.plot(k_values, inertias, marker="o")
plt.xlabel("k"); plt.ylabel("Inertia"); plt.title("KMeans Elbow")
plt.tight_layout(); plt.savefig("output/kmeans_elbow.png", dpi=150); plt.show()

plt.figure(figsize=(10,4))
plt.plot(k_values, sils, marker="o")
plt.xlabel("k"); plt.ylabel("Silhouette"); plt.title("KMeans Silhouette (subsample)")
plt.tight_layout(); plt.savefig("output/kmeans_silhouette.png", dpi=150); plt.show()

best_k = int(k_values[int(np.argmax(sils))])
print("Chosen k:", best_k)

```

# Final fit
```{python}
#| echo: true
#| error: true

from sklearn.decomposition import TruncatedSVD  # used later

kmeans = KMeans(n_clusters=best_k, n_init=20, random_state=RANDOM_STATE)
labels = kmeans.fit_predict(X)

# Build a compact frame with inputs + cluster id
df_clusters = df[[text_col] + cat_cols + num_cols].copy()
df_clusters["cluster"] = labels

# Save outputs for reuse
df_clusters.to_csv("output/cluster_assignments.csv", index=False)

sizes = df_clusters["cluster"].value_counts().sort_index()
print("Cluster sizes:\n", sizes)
sizes.to_csv("output/cluster_sizes.csv")

# --- Top TF-IDF terms per cluster (text portion) ---
tfidf = pre.named_transformers_["txt"].named_steps["tfidf"]
terms = np.array(tfidf.get_feature_names_out())
text_only = tfidf.transform(_clean_text_input(df[text_col]))

top_n = 15
top_terms = {}
for c in range(best_k):
    mask = (labels == c)
    if mask.sum() == 0:
        top_terms[c] = []
        continue
    centroid = text_only[mask].mean(axis=0)
    centroid = np.asarray(centroid).ravel()
    idx = np.argsort(centroid)[::-1][:top_n]
    top_terms[c] = terms[idx].tolist()

with open("output/cluster_top_terms.txt", "w") as f:
    for c in range(best_k):
        f.write(f"Cluster {c} — top terms:\n")
        f.write(", ".join(top_terms[c]) + "\n\n")

print("Top terms file saved -> output/cluster_top_terms.txt")

```

# Cluster summary charts
```{python}
#| echo: true
#| error: true

import matplotlib.patches as mpatches
from matplotlib.ticker import FuncFormatter

# Build dfc in-memory from assignments
dfc = df_clusters.copy()

# Detect AI terms on TITLE
TITLE_COL = "TITLE_CLEAN" if "TITLE_CLEAN" in dfc.columns else "TITLE"
assert TITLE_COL in dfc.columns, "Need a TITLE or TITLE_CLEAN column in dfc."

AI_TERMS = [
    r"\bAI\b", r"\bML\b", r"\bLLM\b", r"\bNLP\b",
    "artificial intelligence", "machine learning", "deep learning",
    "computer vision", "generative", "gen ai", "chatgpt", r"gpt-\d+",
    "transformer", "bert", "prompt engineer", "reinforcement learning"
]
ai_pat = re.compile("|".join(AI_TERMS), flags=re.IGNORECASE)
if "is_ai" not in dfc.columns:
    dfc["is_ai"] = dfc[TITLE_COL].astype(str).str.contains(ai_pat, na=False)

# Colors per cluster id
cluster_colors = {c: plt.cm.get_cmap("tab10")(i % 10) for i, c in enumerate(sorted(dfc["cluster"].unique()))}

# Human-friendly names (fallback to id if not mapped)
cluster_names = {i: f"Cluster {i}" for i in sorted(dfc["cluster"].unique())}
dfc["cluster_name"] = dfc["cluster"].map(cluster_names)

# Aggregate stats
stats = (
    dfc.groupby("cluster", as_index=False)
       .agg(postings=("cluster","size"),
            ai_share=("is_ai","mean"),
            median_salary=("SALARY","median"))
)
stats["cluster_name"] = stats["cluster"].map(cluster_names)
stats["label"] = stats.apply(lambda r: f"C{int(r.cluster)} — {r.cluster_name}", axis=1)
stats["ai_share_pct"] = (stats["ai_share"] * 100).round(1)
stats["color"] = stats["cluster"].map(cluster_colors)

# Legend/key (reusable)
legend_handles = [
    mpatches.Patch(color=cluster_colors[c], label=f"C{c}")
    for c in sorted(stats["cluster"].unique())
]

# Plot 1: AI share by cluster
dfp = stats.sort_values("ai_share_pct", ascending=True)
plt.figure(figsize=(11, 4))
plt.barh(dfp["label"], dfp["ai_share_pct"], color=dfp["color"])
plt.title("AI Share by Cluster (% of postings with AI terms)")
plt.xlabel("AI Share (%)")
for y, v in enumerate(dfp["ai_share_pct"]):
    plt.text(v + 0.5, y, f"{v:.1f}%", va="center")
plt.legend(handles=legend_handles, title="Cluster Key", loc="lower right")
plt.tight_layout()
plt.savefig("output/cluster_ai_share.png", dpi=200, bbox_inches="tight")
plt.show()

# Plot 2: Median salary by cluster
dfp = stats.sort_values("median_salary", ascending=True)
plt.figure(figsize=(11, 4))
plt.barh(dfp["label"], dfp["median_salary"], color=dfp["color"])
plt.title("Median Salary by Cluster")
plt.xlabel("Salary (USD)")
plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f"${int(x):,}"))
for y, v in enumerate(dfp["median_salary"]):
    if pd.notnull(v):
        plt.text(v, y, f"${int(v):,}", va="center", ha="left", fontsize=9)
plt.legend(handles=legend_handles, title="Cluster Key", loc="lower right")
plt.tight_layout()
plt.savefig("output/cluster_median_salary.png", dpi=200, bbox_inches="tight")
plt.show()

# Save tabular summary
stats_out = stats[["cluster","cluster_name","postings","ai_share_pct","median_salary"]].sort_values("cluster")
stats_out.to_csv("output/cluster_summary.csv", index=False)
stats_out.head(10)

```

# SVD 2D
```{python}
#| label: svd-2d
#| echo: true
#| error: true

from sklearn.decomposition import TruncatedSVD

assert "X" in globals(), "Feature matrix X missing."
svd = TruncatedSVD(n_components=2, random_state=RANDOM_STATE)
XY = svd.fit_transform(X)
print("Explained variance (2 comps):", svd.explained_variance_ratio_.sum())


```


# Single Cluster Scatter
```{python}
#| echo: true
#| error: true
#| dependson: [svd-2d]

plt.figure(figsize=(9,6))
for c in sorted(np.unique(labels)):
    m = (labels == c)
    plt.scatter(XY[m,0], XY[m,1], s=8, alpha=0.5,
                color=cluster_colors[c], label=f"C{c}")
plt.title("KMeans clusters (2-D SVD embedding)")
plt.xlabel("SVD 1"); plt.ylabel("SVD 2")
plt.legend(markerscale=2, frameon=True)
plt.tight_layout()
plt.savefig("output/kmeans_svd_scatter.png", dpi=180, bbox_inches="tight")
plt.show()

```

Under the recommendation of the Kmeans Elbow and Silhouette measures, four clear segments have emerged. C0 (EA / SAP–Oracle Consulting, Sr) comprises enterprise solution owners and senior consultants focused on ERP/CRM integrations, domain architecture, and delivery roadmaps. C1 (Data / BI Analysts) is the high‑volume analytics backbone handling reporting, dashboards, and KPI/ad‑hoc analysis at mid‑career compensation. C2 (Enterprise / Cloud Architects) is the premium niche cluster with principals and leads who own cloud platforms, reliability/security, and cross‑team technical direction, and therefore command the highest pay. C3 (Data / BI Analysts, consulting tilt) mirrors C1’s skills but skews toward consulting and remote work and shows the highest AI‑keyword incidence, reflecting applied‑AI enablement inside analytics teams. Overall, analyst demand drives scale (C1/C3), enterprise solutioning provides the integration bench (C0), and cross‑platform leadership remains scarce and premium (C2).

# Reference table
```{python}
#| echo: true
#| error: true

from sklearn.metrics import (
    adjusted_rand_score, normalized_mutual_info_score,
    homogeneity_score, completeness_score, v_measure_score
)

# Choose a reasonable reference column 
REF_COL = next((c for c in [
    "SOC_2021_3_NAME","SOC_2021_2_NAME","SOC_2021_5_NAME",
    "NAICS_2022_4_NAME","NAICS_2022_2_NAME","NAICS_2022_6_NAME",
    "ONET_NAME","ONET_2019_NAME"
] if c in df.columns), None)
assert REF_COL, "No SOC/NAICS/ONET label column found."

df_clusters = df_clusters.copy()
df_clusters["ref_label"] = df[REF_COL].astype("string").fillna("Unknown").values

y_true = df_clusters["ref_label"].astype(str).values
y_pred = df_clusters["cluster"].astype(int).values

nmi  = normalized_mutual_info_score(y_true, y_pred)
ari  = adjusted_rand_score(y_true, y_pred)
hom  = homogeneity_score(y_true, y_pred)
comp = completeness_score(y_true, y_pred)
vms  = v_measure_score(y_true, y_pred)

print(f"NMI: {nmi:.3f} | ARI: {ari:.3f} | Homogeneity: {hom:.3f} | Completeness: {comp:.3f} | V-measure: {vms:.3f}")

# Majority label per cluster + purity
ct = pd.crosstab(df_clusters["cluster"], df_clusters["ref_label"])
cluster_major = ct.idxmax(axis=1).rename("majority_label")
cluster_hits  = ct.max(axis=1)
purity = cluster_hits.sum() / ct.values.sum()

summary = (
    pd.concat([cluster_major, cluster_hits.rename("majority_count"),
               ct.sum(axis=1).rename("cluster_size")], axis=1)
      .assign(majority_share=lambda d: d["majority_count"] / d["cluster_size"])
      .sort_index()
)

print("\nCluster → majority reference label:")
summary.head(best_k)
print(f"\nOverall purity: {purity:.3f}")

summary.to_csv("output/cluster_majority_label_summary.csv")
ct.to_csv("output/cluster_label_crosstab.csv")

```

# Heatmap prep
```{python}
#| echo: true
#| error: true

import pandas as pd

# Quick diagnostics for label visibility
cands = [
    "NAICS_2022_6_NAME","NAICS_2022_4_NAME","NAICS_2022_2_NAME",
    "ONET_NAME","ONET_2019_NAME",
    "SOC_2021_5_NAME","SOC_2021_3_NAME","SOC_2021_2_NAME",
]
diag = []
for c in cands:
    if c in df.columns:
        s = df[c].astype("string")
        diag.append({
            "column": c,
            "non_null_share": s.notna().mean(),
            "n_unique_nonnull": s.dropna().nunique(),
            "top5": s.value_counts(dropna=True).head(5).index.tolist()
        })
diag_df = pd.DataFrame(diag).sort_values(["n_unique_nonnull","non_null_share"], ascending=[False, False])
print("Label candidates (more uniques is better):")
display(diag_df)

# --- Auto-pick with stronger uniqueness requirement ---
# Prefer columns with decent coverage and at least 5–40 distinct values
viable = diag_df[(diag_df["non_null_share"] >= 0.40) & (diag_df["n_unique_nonnull"].between(5, 40))]
if len(viable):
    REF_COL = viable.iloc[0]["column"]
else:
    # Fallback
    REF_COL = diag_df.iloc[0]["column"]


print("Using reference label column:", REF_COL)

```

# Heatmap plot
```{python}
#| echo: true
#| error: true

import numpy as np, pandas as pd, os
import matplotlib.pyplot as plt
try:
    import seaborn as sns
    use_sns = True
except Exception:
    use_sns = False

os.makedirs("output", exist_ok=True)

# Base clustered frame
if "dfc" in globals():
    base = dfc.copy()
elif "df_clusters" in globals():
    base = df_clusters.copy()
else:
    raise AssertionError("Run the KMeans chunk so dfc/df_clusters exist.")

# Attach chosen reference label (aligned by row order)
base["ref_label"] = df[REF_COL].astype("string").fillna("Unknown").values

# Crosstab clusters x labels
ct = pd.crosstab(base["cluster"], base["ref_label"])

# Drop 'Unknown' column if present
if "Unknown" in ct.columns:
    ct = ct.drop(columns=["Unknown"])

# Keep top N labels overall (bucket the rest)
TOPN = 15
if ct.shape[1] > TOPN:
    keep = ct.sum(axis=0).sort_values(ascending=False).head(TOPN).index
    ct_reduced = ct[keep].copy()
    ct_reduced["Other"] = ct.drop(columns=keep).sum(axis=1)
else:
    ct_reduced = ct

# Remove empty rows (just in case)
ct_reduced = ct_reduced.loc[ct_reduced.sum(axis=1) > 0]

# Convert to row percentages
pct = ct_reduced.div(ct_reduced.sum(axis=1), axis=0) * 100


if pct.shape[1] <= 1:
    print(f" {REF_COL} doesn’t have enough diversity after cleaning. "
          f"Try overriding REF_COL to something like NAICS_2022_4_NAME or ONET_NAME.")
else:
    plt.figure(figsize=(max(10, 0.7*pct.shape[1]), max(4, 0.6*pct.shape[0])))
    if use_sns:
        ax = sns.heatmap(pct, cmap="Blues", annot=True, fmt=".0f",
                         cbar_kws={"label": "% within cluster"})
    else:
        im = plt.imshow(pct.values, aspect="auto", cmap="Blues")
        plt.colorbar(im, label="% within cluster")
        plt.xticks(range(pct.shape[1]), pct.columns, rotation=45, ha="right")
        plt.yticks(range(pct.shape[0]), pct.index)
        ax = plt.gca()

    plt.title(f"Cluster vs {REF_COL} (row %)")
    plt.xlabel(REF_COL); plt.ylabel("Cluster")
    plt.tight_layout()
    plt.savefig("output/cluster_vs_reference_heatmap.png", dpi=200, bbox_inches="tight")
    plt.show()

```
This figure is a heatmap of job‑posting shares by industry, displayed as row percentages. The leftmost column— that includes Professional, Scientific & Technical Services, shows the darkest shading overall, meaning a large portion of postings in each row come from that industry, while lighter columns represent industries that account for a smaller share. The Profesional, Scientific, and Technical Services industry does seem to be the hottest job market based off of this dataset, however, this does not mean that it is necessarily driven by AI alone.

# Kmeans Summary
The results point to a job market organized around four role families with one noticeably premium niche, and they suggest that compensation and hiring dynamics are driven more by seniority and enterprise scope than by AI keywords. Cluster C1 (Analyst & BI Core) supplies most of the volume (≈56%), centered on titles like data analyst, business analyst, and BI/analytics; it anchors day‑to‑day decision support and sits near the mid‑$110k median. Cluster C3 (Data Analyst & BI — Mixed/Remote, AI‑skewed) is a smaller analyst cohort with a stronger remote profile and the highest AI‑keyword incidence; it reflects applied‑AI enablement inside analytics teams and also prices around the mid‑$110k band. Cluster C0 (Enterprise Architecture & Solutions) blends architect/enterprise/SAP‑Oracle/consultant language and tilts toward cross‑system solution ownership—ERP/CRM modernization, integration roadmaps, and domain architecture—with compensation likewise clustering near the mid‑$110k range. Finally, Cluster C2 (Senior Enterprise/Cloud Leadership) is the small but premium niche (~7%) that stands apart both visually and economically, with a roughly $175k median in the provided charts; employers reward systems ownership, architectural accountability, reliability/security considerations, and cross‑team leadership, even when titles don’t carry explicit AI keywords. Meanwhile, AI‑tagged titles are increasing from a small base and are unevenly distributed—most common in C3 and least common in C2, yet the salary premium does not follow that same AI gradient, reinforcing that pay aligns with role family and enterprise scope rather than AI labeling. Industry composition across all clusters is anchored in Professional, Scientific & Technical Services, with steady contributions from Administrative & Support, Finance & Insurance, and Manufacturing. The fastest AI growth is occurring outside of pure software—holding companies, motor‑vehicle parts, R&D services, executive search, and temporary help which signals diffusion of AI demand into corporate centers and industrial supply chains. This suggests that an increased use of applied AI in operational workforces. Altogether, the evidence suggests a hybrid hiring mix: steady growth of applied‑AI roles embedded in analytics workflows (C1/C3), alongside continued scarcity and premium pricing for cross‑functional platform leaders (C2) who can translate strategy into architecture and delivery, with C0 providing the enterprise solutions bench that stitches systems and governance together. Although AI is a key component within this analysis, it does not necessarily mean that AI is overtaking the job industry and 'destroying the job market', but rather remodeling the job market and how we use technology within the workplace. AI is steadily contributing to industries such as Administrative & Support, Finance & Insurance, and Manufacturing, however, premium job postings with the highest salaries seem to still value human judgement and ability.

